==== START logs for container kube-controller-manager of pod kube-system/kube-controller-manager-cloudtest-kind-16b381ca64-control-plane ====
I0611 16:09:13.784767       1 serving.go:312] Generated self-signed cert in-memory
I0611 16:09:14.995245       1 controllermanager.go:161] Version: v1.17.0
I0611 16:09:14.995953       1 dynamic_cafile_content.go:166] Starting request-header::/etc/kubernetes/pki/front-proxy-ca.crt
I0611 16:09:14.996131       1 dynamic_cafile_content.go:166] Starting client-ca-bundle::/etc/kubernetes/pki/ca.crt
I0611 16:09:14.996817       1 secure_serving.go:178] Serving securely on 127.0.0.1:10257
I0611 16:09:14.996876       1 tlsconfig.go:219] Starting DynamicServingCertificateController
I0611 16:09:14.997216       1 deprecated_insecure_serving.go:53] Serving insecurely on [::]:10252
I0611 16:09:14.997355       1 leaderelection.go:242] attempting to acquire leader lease  kube-system/kube-controller-manager...
E0611 16:09:20.592726       1 leaderelection.go:331] error retrieving resource lock kube-system/kube-controller-manager: endpoints "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "endpoints" in API group "" in the namespace "kube-system"
I0611 16:09:24.426102       1 leaderelection.go:252] successfully acquired lease kube-system/kube-controller-manager
I0611 16:09:24.426135       1 event.go:281] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"4289a6d9-d300-4c2b-893f-1577ec1ccba7", APIVersion:"v1", ResourceVersion:"189", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' cloudtest-kind-16b381ca64-control-plane_7cbd78c6-3564-4620-a85e-f2cdd06493b8 became leader
I0611 16:09:24.426546       1 event.go:281] Event(v1.ObjectReference{Kind:"Lease", Namespace:"kube-system", Name:"kube-controller-manager", UID:"57d4cf78-9da5-49d1-80d3-c7e94b542ea4", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"190", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' cloudtest-kind-16b381ca64-control-plane_7cbd78c6-3564-4620-a85e-f2cdd06493b8 became leader
I0611 16:09:24.686820       1 plugins.go:100] No cloud provider specified.
I0611 16:09:24.687742       1 shared_informer.go:197] Waiting for caches to sync for tokens
I0611 16:09:24.789960       1 shared_informer.go:204] Caches are synced for tokens 
E0611 16:09:24.826132       1 core.go:91] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail
W0611 16:09:24.826203       1 controllermanager.go:525] Skipping "service"
I0611 16:09:24.889455       1 controllermanager.go:533] Started "namespace"
I0611 16:09:24.889528       1 namespace_controller.go:200] Starting namespace controller
I0611 16:09:24.889553       1 shared_informer.go:197] Waiting for caches to sync for namespace
I0611 16:09:25.447879       1 controllermanager.go:533] Started "garbagecollector"
I0611 16:09:25.448044       1 garbagecollector.go:129] Starting garbage collector controller
I0611 16:09:25.448078       1 shared_informer.go:197] Waiting for caches to sync for garbage collector
I0611 16:09:25.448157       1 graph_builder.go:282] GraphBuilder running
I0611 16:09:25.490056       1 controllermanager.go:533] Started "csrsigning"
I0611 16:09:25.490451       1 certificate_controller.go:118] Starting certificate controller "csrsigning"
I0611 16:09:25.490483       1 shared_informer.go:197] Waiting for caches to sync for certificate-csrsigning
I0611 16:09:25.541359       1 controllermanager.go:533] Started "serviceaccount"
I0611 16:09:25.541603       1 serviceaccounts_controller.go:116] Starting service account controller
I0611 16:09:25.541767       1 shared_informer.go:197] Waiting for caches to sync for service account
I0611 16:09:25.588489       1 controllermanager.go:533] Started "persistentvolume-binder"
I0611 16:09:25.589030       1 pv_controller_base.go:294] Starting persistent volume controller
I0611 16:09:25.589100       1 shared_informer.go:197] Waiting for caches to sync for persistent volume
I0611 16:09:25.632169       1 controllermanager.go:533] Started "attachdetach"
I0611 16:09:25.632207       1 attach_detach_controller.go:342] Starting attach detach controller
I0611 16:09:25.632246       1 shared_informer.go:197] Waiting for caches to sync for attach detach
I0611 16:09:25.688006       1 controllermanager.go:533] Started "pvc-protection"
I0611 16:09:25.688400       1 pvc_protection_controller.go:100] Starting PVC protection controller
I0611 16:09:25.688429       1 shared_informer.go:197] Waiting for caches to sync for PVC protection
I0611 16:09:25.734171       1 controllermanager.go:533] Started "endpoint"
I0611 16:09:25.734396       1 endpoints_controller.go:181] Starting endpoint controller
I0611 16:09:25.734618       1 shared_informer.go:197] Waiting for caches to sync for endpoint
I0611 16:09:25.844526       1 controllermanager.go:533] Started "replicaset"
W0611 16:09:25.844609       1 controllermanager.go:525] Skipping "ttl-after-finished"
I0611 16:09:25.844775       1 replica_set.go:180] Starting replicaset controller
I0611 16:09:25.844806       1 shared_informer.go:197] Waiting for caches to sync for ReplicaSet
I0611 16:09:26.093802       1 controllermanager.go:533] Started "tokencleaner"
W0611 16:09:26.094296       1 core.go:246] configure-cloud-routes is set, but no cloud provider specified. Will not configure cloud provider routes.
I0611 16:09:26.094000       1 tokencleaner.go:117] Starting token cleaner controller
W0611 16:09:26.094406       1 controllermanager.go:525] Skipping "route"
I0611 16:09:26.094461       1 shared_informer.go:197] Waiting for caches to sync for token_cleaner
I0611 16:09:26.094579       1 shared_informer.go:204] Caches are synced for token_cleaner 
I0611 16:09:26.343288       1 clusterroleaggregation_controller.go:148] Starting ClusterRoleAggregator
I0611 16:09:26.343561       1 shared_informer.go:197] Waiting for caches to sync for ClusterRoleAggregator
I0611 16:09:26.343371       1 controllermanager.go:533] Started "clusterrole-aggregation"
W0611 16:09:26.343705       1 controllermanager.go:525] Skipping "endpointslice"
I0611 16:09:26.592521       1 controllermanager.go:533] Started "replicationcontroller"
I0611 16:09:26.592910       1 replica_set.go:180] Starting replicationcontroller controller
I0611 16:09:26.593329       1 shared_informer.go:197] Waiting for caches to sync for ReplicationController
I0611 16:09:27.293459       1 controllermanager.go:533] Started "horizontalpodautoscaling"
I0611 16:09:27.293565       1 horizontal.go:156] Starting HPA controller
I0611 16:09:27.293938       1 shared_informer.go:197] Waiting for caches to sync for HPA
I0611 16:09:27.443018       1 controllermanager.go:533] Started "csrcleaner"
I0611 16:09:27.443238       1 cleaner.go:81] Starting CSR cleaner controller
I0611 16:09:27.693953       1 node_lifecycle_controller.go:77] Sending events to api server
E0611 16:09:27.694039       1 core.go:232] failed to start cloud node lifecycle controller: no cloud provider provided
W0611 16:09:27.694114       1 controllermanager.go:525] Skipping "cloud-node-lifecycle"
I0611 16:09:27.943733       1 controllermanager.go:533] Started "persistentvolume-expander"
I0611 16:09:27.944371       1 expand_controller.go:319] Starting expand controller
I0611 16:09:27.944526       1 shared_informer.go:197] Waiting for caches to sync for expand
I0611 16:09:28.192119       1 controllermanager.go:533] Started "podgc"
I0611 16:09:28.192307       1 gc_controller.go:88] Starting GC controller
I0611 16:09:28.192356       1 shared_informer.go:197] Waiting for caches to sync for GC
I0611 16:09:28.696113       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for deployments.apps
I0611 16:09:28.696214       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for ingresses.networking.k8s.io
I0611 16:09:28.696269       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
I0611 16:09:28.696320       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for endpoints
I0611 16:09:28.696401       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for podtemplates
I0611 16:09:28.696475       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for statefulsets.apps
I0611 16:09:28.696526       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for jobs.batch
I0611 16:09:28.696672       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
I0611 16:09:28.696810       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for events.events.k8s.io
I0611 16:09:28.696885       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for cronjobs.batch
I0611 16:09:28.696938       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
I0611 16:09:28.696983       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for limitranges
W0611 16:09:28.697016       1 shared_informer.go:415] resyncPeriod 49087481051849 is smaller than resyncCheckPeriod 77833783610029 and the informer has already started. Changing it to 77833783610029
I0611 16:09:28.697112       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for serviceaccounts
I0611 16:09:28.697200       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
I0611 16:09:28.697379       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
I0611 16:09:28.697470       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for controllerrevisions.apps
I0611 16:09:28.697534       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for ingresses.extensions
I0611 16:09:28.697589       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for daemonsets.apps
I0611 16:09:28.697678       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for endpointslices.discovery.k8s.io
W0611 16:09:28.697774       1 shared_informer.go:415] resyncPeriod 56723227991456 is smaller than resyncCheckPeriod 77833783610029 and the informer has already started. Changing it to 77833783610029
I0611 16:09:28.697945       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for replicasets.apps
I0611 16:09:28.698065       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
I0611 16:09:28.698121       1 controllermanager.go:533] Started "resourcequota"
I0611 16:09:28.698534       1 resource_quota_controller.go:271] Starting resource quota controller
I0611 16:09:28.698565       1 shared_informer.go:197] Waiting for caches to sync for resource quota
I0611 16:09:28.698665       1 resource_quota_monitor.go:303] QuotaMonitor running
I0611 16:09:28.734131       1 controllermanager.go:533] Started "job"
I0611 16:09:28.734291       1 job_controller.go:143] Starting job controller
I0611 16:09:28.734315       1 shared_informer.go:197] Waiting for caches to sync for job
I0611 16:09:29.056979       1 controllermanager.go:533] Started "disruption"
I0611 16:09:29.057115       1 disruption.go:330] Starting disruption controller
I0611 16:09:29.057136       1 shared_informer.go:197] Waiting for caches to sync for disruption
I0611 16:09:29.307103       1 controllermanager.go:533] Started "deployment"
I0611 16:09:29.307149       1 deployment_controller.go:152] Starting deployment controller
I0611 16:09:29.307175       1 shared_informer.go:197] Waiting for caches to sync for deployment
I0611 16:09:29.457059       1 node_ipam_controller.go:94] Sending events to api server.
I0611 16:09:39.463302       1 range_allocator.go:82] Sending events to api server.
I0611 16:09:39.463424       1 range_allocator.go:116] No Secondary Service CIDR provided. Skipping filtering out secondary service addresses.
I0611 16:09:39.463470       1 controllermanager.go:533] Started "nodeipam"
I0611 16:09:39.463741       1 node_ipam_controller.go:162] Starting ipam controller
I0611 16:09:39.463793       1 shared_informer.go:197] Waiting for caches to sync for node
I0611 16:09:39.475689       1 node_lifecycle_controller.go:388] Sending events to api server.
I0611 16:09:39.475835       1 node_lifecycle_controller.go:423] Controller is using taint based evictions.
I0611 16:09:39.476251       1 taint_manager.go:162] Sending events to api server.
I0611 16:09:39.476486       1 node_lifecycle_controller.go:520] Controller will reconcile labels.
I0611 16:09:39.476597       1 controllermanager.go:533] Started "nodelifecycle"
I0611 16:09:39.476815       1 node_lifecycle_controller.go:554] Starting node controller
I0611 16:09:39.476869       1 shared_informer.go:197] Waiting for caches to sync for taint
I0611 16:09:39.527233       1 controllermanager.go:533] Started "ttl"
I0611 16:09:39.527288       1 ttl_controller.go:116] Starting TTL controller
I0611 16:09:39.527889       1 shared_informer.go:197] Waiting for caches to sync for TTL
I0611 16:09:39.558993       1 controllermanager.go:533] Started "bootstrapsigner"
I0611 16:09:39.559093       1 shared_informer.go:197] Waiting for caches to sync for bootstrap_signer
I0611 16:09:39.618077       1 controllermanager.go:533] Started "pv-protection"
W0611 16:09:39.618160       1 controllermanager.go:525] Skipping "root-ca-cert-publisher"
I0611 16:09:39.618918       1 pv_protection_controller.go:81] Starting PV protection controller
I0611 16:09:39.619029       1 shared_informer.go:197] Waiting for caches to sync for PV protection
I0611 16:09:39.646582       1 controllermanager.go:533] Started "daemonset"
I0611 16:09:39.647031       1 daemon_controller.go:255] Starting daemon sets controller
I0611 16:09:39.647105       1 shared_informer.go:197] Waiting for caches to sync for daemon sets
I0611 16:09:39.671337       1 controllermanager.go:533] Started "statefulset"
I0611 16:09:39.671813       1 stateful_set.go:145] Starting stateful set controller
I0611 16:09:39.671861       1 shared_informer.go:197] Waiting for caches to sync for stateful set
I0611 16:09:39.693747       1 controllermanager.go:533] Started "cronjob"
I0611 16:09:39.693796       1 cronjob_controller.go:97] Starting CronJob Manager
I0611 16:09:39.767135       1 controllermanager.go:533] Started "csrapproving"
I0611 16:09:39.767685       1 certificate_controller.go:118] Starting certificate controller "csrapproving"
I0611 16:09:39.767732       1 shared_informer.go:197] Waiting for caches to sync for certificate-csrapproving
I0611 16:09:39.770458       1 shared_informer.go:197] Waiting for caches to sync for resource quota
I0611 16:09:39.770782       1 shared_informer.go:197] Waiting for caches to sync for garbage collector
I0611 16:09:39.809140       1 shared_informer.go:204] Caches are synced for ClusterRoleAggregator 
I0611 16:09:39.819444       1 shared_informer.go:204] Caches are synced for PV protection 
I0611 16:09:39.855757       1 shared_informer.go:204] Caches are synced for certificate-csrsigning 
I0611 16:09:39.860080       1 shared_informer.go:204] Caches are synced for bootstrap_signer 
I0611 16:09:39.868079       1 shared_informer.go:204] Caches are synced for certificate-csrapproving 
I0611 16:09:39.953878       1 shared_informer.go:204] Caches are synced for PVC protection 
I0611 16:09:39.959168       1 shared_informer.go:204] Caches are synced for ReplicationController 
I0611 16:09:39.959585       1 shared_informer.go:204] Caches are synced for HPA 
W0611 16:09:39.971937       1 actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="cloudtest-kind-16b381ca64-control-plane" does not exist
I0611 16:09:39.975210       1 shared_informer.go:204] Caches are synced for stateful set 
I0611 16:09:39.979177       1 shared_informer.go:204] Caches are synced for taint 
I0611 16:09:39.979466       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
W0611 16:09:39.979574       1 node_lifecycle_controller.go:1058] Missing timestamp for Node cloudtest-kind-16b381ca64-control-plane. Assuming now as a timestamp.
I0611 16:09:39.980015       1 node_lifecycle_controller.go:1209] Controller detected that all Nodes are not-Ready. Entering master disruption mode.
I0611 16:09:39.981038       1 taint_manager.go:186] Starting NoExecuteTaintManager
I0611 16:09:39.981709       1 event.go:281] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"cloudtest-kind-16b381ca64-control-plane", UID:"ecee1cc1-b878-4892-975b-40cf3bec72c6", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node cloudtest-kind-16b381ca64-control-plane event: Registered Node cloudtest-kind-16b381ca64-control-plane in Controller
I0611 16:09:39.999814       1 shared_informer.go:204] Caches are synced for job 
I0611 16:09:40.010754       1 shared_informer.go:204] Caches are synced for ReplicaSet 
I0611 16:09:40.028605       1 shared_informer.go:204] Caches are synced for TTL 
I0611 16:09:40.047794       1 shared_informer.go:204] Caches are synced for daemon sets 
I0611 16:09:40.054857       1 shared_informer.go:204] Caches are synced for namespace 
I0611 16:09:40.060774       1 shared_informer.go:204] Caches are synced for GC 
I0611 16:09:40.065774       1 shared_informer.go:204] Caches are synced for node 
I0611 16:09:40.065932       1 range_allocator.go:172] Starting range CIDR allocator
I0611 16:09:40.065984       1 shared_informer.go:197] Waiting for caches to sync for cidrallocator
I0611 16:09:40.066021       1 shared_informer.go:204] Caches are synced for cidrallocator 
I0611 16:09:40.084107       1 range_allocator.go:373] Set node cloudtest-kind-16b381ca64-control-plane PodCIDR to [10.244.0.0/24]
I0611 16:09:40.088730       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kindnet", UID:"8bae77f6-fd09-456d-8c8c-ff29cf0491f6", APIVersion:"apps/v1", ResourceVersion:"247", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kindnet-g2z7s
I0611 16:09:40.091538       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-proxy", UID:"9f4b498a-3e8a-4b1e-9c3b-58fcac5a0c81", APIVersion:"apps/v1", ResourceVersion:"185", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-qh6vz
I0611 16:09:40.097910       1 shared_informer.go:204] Caches are synced for attach detach 
I0611 16:09:40.108102       1 shared_informer.go:204] Caches are synced for service account 
E0611 16:09:40.135237       1 daemon_controller.go:290] kube-system/kube-proxy failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy", UID:"9f4b498a-3e8a-4b1e-9c3b-58fcac5a0c81", ResourceVersion:"185", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759024564, loc:(*time.Location)(0x3992980)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc0014b4ac0), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-proxy", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc0010724c0), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"xtables-lock", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc0014b4ae0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"lib-modules", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc0014b4b00), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"kube-proxy", Image:"k8s.gcr.io/kube-proxy:v1.17.0", Command:[]string{"/usr/local/bin/kube-proxy", "--config=/var/lib/kube-proxy/config.conf", "--hostname-override=$(NODE_NAME)"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc0014b4b40)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-proxy", ReadOnly:false, MountPath:"/var/lib/kube-proxy", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"xtables-lock", ReadOnly:false, MountPath:"/run/xtables.lock", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"lib-modules", ReadOnly:true, MountPath:"/lib/modules", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc001076ff0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0009d89d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"beta.kubernetes.io/os":"linux"}, ServiceAccountName:"kube-proxy", DeprecatedServiceAccount:"kube-proxy", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a63620), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"CriticalAddonsOnly", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"system-node-critical", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc000278818)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc0009d8a18)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kube-proxy": the object has been modified; please apply your changes to the latest version and try again
I0611 16:09:40.154321       1 shared_informer.go:204] Caches are synced for persistent volume 
I0611 16:09:40.209778       1 shared_informer.go:204] Caches are synced for expand 
I0611 16:09:40.300018       1 shared_informer.go:204] Caches are synced for endpoint 
I0611 16:09:40.307536       1 shared_informer.go:204] Caches are synced for deployment 
I0611 16:09:40.316724       1 event.go:281] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"local-path-storage", Name:"local-path-provisioner", UID:"282f8e3e-b2b9-4177-b4b3-8b306457ecd7", APIVersion:"apps/v1", ResourceVersion:"267", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set local-path-provisioner-7745554f7f to 1
I0611 16:09:40.322696       1 event.go:281] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"coredns-6955765f44", UID:"5c8db1a1-64b4-496d-b3d8-bf091d24905d", APIVersion:"apps/v1", ResourceVersion:"365", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: coredns-6955765f44-55sgd
I0611 16:09:40.329327       1 event.go:281] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"coredns", UID:"d47d4804-da60-43d1-a89c-302d0942af29", APIVersion:"apps/v1", ResourceVersion:"180", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set coredns-6955765f44 to 2
I0611 16:09:40.348226       1 event.go:281] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"coredns-6955765f44", UID:"5c8db1a1-64b4-496d-b3d8-bf091d24905d", APIVersion:"apps/v1", ResourceVersion:"365", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: coredns-6955765f44-fwc4g
I0611 16:09:40.360059       1 shared_informer.go:204] Caches are synced for disruption 
I0611 16:09:40.360151       1 disruption.go:338] Sending events to api server.
I0611 16:09:40.365645       1 shared_informer.go:204] Caches are synced for resource quota 
I0611 16:09:40.372619       1 shared_informer.go:204] Caches are synced for garbage collector 
I0611 16:09:40.372750       1 shared_informer.go:204] Caches are synced for resource quota 
I0611 16:09:40.401483       1 event.go:281] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"local-path-storage", Name:"local-path-provisioner-7745554f7f", UID:"baa881d6-f35b-4b1d-8822-c7dceb8830df", APIVersion:"apps/v1", ResourceVersion:"366", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: local-path-provisioner-7745554f7f-7zh68
I0611 16:09:40.415867       1 shared_informer.go:204] Caches are synced for garbage collector 
I0611 16:09:40.415921       1 garbagecollector.go:138] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
W0611 16:10:01.156322       1 actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="cloudtest-kind-16b381ca64-worker" does not exist
I0611 16:10:01.183599       1 range_allocator.go:373] Set node cloudtest-kind-16b381ca64-worker PodCIDR to [10.244.1.0/24]
I0611 16:10:01.206102       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-proxy", UID:"9f4b498a-3e8a-4b1e-9c3b-58fcac5a0c81", APIVersion:"apps/v1", ResourceVersion:"453", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-jm8qh
I0611 16:10:01.210262       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kindnet", UID:"8bae77f6-fd09-456d-8c8c-ff29cf0491f6", APIVersion:"apps/v1", ResourceVersion:"451", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kindnet-h5nx2
W0611 16:10:01.298396       1 actual_state_of_world.go:506] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="cloudtest-kind-16b381ca64-worker2" does not exist
I0611 16:10:01.348054       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kindnet", UID:"8bae77f6-fd09-456d-8c8c-ff29cf0491f6", APIVersion:"apps/v1", ResourceVersion:"501", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kindnet-c77nc
I0611 16:10:01.352268       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-proxy", UID:"9f4b498a-3e8a-4b1e-9c3b-58fcac5a0c81", APIVersion:"apps/v1", ResourceVersion:"494", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-c5zhh
I0611 16:10:01.380272       1 range_allocator.go:373] Set node cloudtest-kind-16b381ca64-worker2 PodCIDR to [10.244.2.0/24]
W0611 16:10:04.956829       1 node_lifecycle_controller.go:1058] Missing timestamp for Node cloudtest-kind-16b381ca64-worker. Assuming now as a timestamp.
W0611 16:10:04.957098       1 node_lifecycle_controller.go:1058] Missing timestamp for Node cloudtest-kind-16b381ca64-worker2. Assuming now as a timestamp.
I0611 16:10:04.957144       1 event.go:281] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"cloudtest-kind-16b381ca64-worker2", UID:"84dc3592-4807-4e73-8c23-cd0edaaad590", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node cloudtest-kind-16b381ca64-worker2 event: Registered Node cloudtest-kind-16b381ca64-worker2 in Controller
I0611 16:10:04.957180       1 event.go:281] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"cloudtest-kind-16b381ca64-worker", UID:"46a0fe04-82b6-46be-ba53-7ddba4beddf8", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node cloudtest-kind-16b381ca64-worker event: Registered Node cloudtest-kind-16b381ca64-worker in Controller
I0611 16:10:24.961292       1 node_lifecycle_controller.go:1236] Controller detected that some Nodes are Ready. Exiting master disruption mode.
I0611 16:12:49.065892       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"spire", Name:"spire-agent", UID:"afc9db50-4372-4a44-b814-c4a8479a98a7", APIVersion:"apps/v1", ResourceVersion:"1117", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: spire-agent-4dzph
I0611 16:12:49.066130       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"spire", Name:"spire-agent", UID:"afc9db50-4372-4a44-b814-c4a8479a98a7", APIVersion:"apps/v1", ResourceVersion:"1117", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: spire-agent-n57qs
I0611 16:12:49.066361       1 event.go:281] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"spire", Name:"spire-agent", UID:"afc9db50-4372-4a44-b814-c4a8479a98a7", APIVersion:"apps/v1", ResourceVersion:"1117", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: spire-agent-srj2s
I0611 16:12:49.166592       1 event.go:281] Event(v1.ObjectReference{Kind:"StatefulSet", Namespace:"spire", Name:"spire-server", UID:"ff802297-e425-499a-9254-ca7ddd03f7ba", APIVersion:"apps/v1", ResourceVersion:"1120", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' create Pod spire-server-0 in StatefulSet spire-server successful
E0611 16:13:08.180269       1 clusterroleaggregation_controller.go:180] view failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "view": the object has been modified; please apply your changes to the latest version and try again
E0611 16:13:08.189482       1 clusterroleaggregation_controller.go:180] admin failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "admin": the object has been modified; please apply your changes to the latest version and try again
E0611 16:13:08.197305       1 clusterroleaggregation_controller.go:180] edit failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "edit": the object has been modified; please apply your changes to the latest version and try again
I0611 16:13:11.959439       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkserviceendpoints.networkservicemesh.io
I0611 16:13:11.959517       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkservicemanagers.networkservicemesh.io
I0611 16:13:11.959766       1 resource_quota_monitor.go:228] QuotaMonitor created object count evaluator for networkservices.networkservicemesh.io
I0611 16:13:11.959846       1 shared_informer.go:197] Waiting for caches to sync for resource quota
I0611 16:13:12.160765       1 shared_informer.go:204] Caches are synced for resource quota 
I0611 16:13:12.282334       1 shared_informer.go:197] Waiting for caches to sync for garbage collector
I0611 16:13:12.282838       1 shared_informer.go:204] Caches are synced for garbage collector 
E0611 16:13:21.797474       1 clusterroleaggregation_controller.go:180] edit failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "edit": the object has been modified; please apply your changes to the latest version and try again
I0611 16:14:31.127339       1 namespace_controller.go:185] Namespace has been deleted nsm-system
E0611 16:14:53.744503       1 clusterroleaggregation_controller.go:180] admin failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "admin": the object has been modified; please apply your changes to the latest version and try again
E0611 16:14:53.747004       1 clusterroleaggregation_controller.go:180] edit failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "edit": the object has been modified; please apply your changes to the latest version and try again
E0611 16:16:14.776170       1 clusterroleaggregation_controller.go:180] admin failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "admin": the object has been modified; please apply your changes to the latest version and try again
E0611 16:16:17.999472       1 clusterroleaggregation_controller.go:180] edit failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "edit": the object has been modified; please apply your changes to the latest version and try again
==== END logs for container kube-controller-manager of pod kube-system/kube-controller-manager-cloudtest-kind-16b381ca64-control-plane ====
