LOGS TOTAL
  75

ERRORS TOTAL
  683

ERRORS COUNT

  182 ./kind-1.kubeconfig/kube-system/kube-controller-manager-kind-1-control-plane/logs.txt
  64 ./kind-2.kubeconfig/wcm-system/wcm-nse-discovery-6fd76df87f-mz5qr/logs.txt
  57 ./kind-3.kubeconfig/wcm-system/wcm-nse-discovery-5f68d85698-2xkfn/logs.txt
  51 ./kind-2.kubeconfig/spire/wcm-spire-agent-rmhtd/logs.txt
  45 ./kind-3.kubeconfig/spire/wcm-spire-agent-bj8b2/logs.txt
  32 ./kind-1.kubeconfig/wcm-system/green-bfd874d66-4s557/logs.txt
  23 ./kind-3.kubeconfig/default/member-core-operator-d598d4888-p7h58/logs.txt
  21 ./kind-2.kubeconfig/default/member-core-operator-d598d4888-xd9cx/logs.txt
  20 ./kind-2.kubeconfig/wcm-system/vl3-nse-green-76bc7f4b96-wg7rx/logs.txt
  20 ./kind-2.kubeconfig/wcm-system/vl3-nse-green-76bc7f4b96-6qz9k/logs.txt
  19 ./kind-3.kubeconfig/wcm-system/vl3-nse-green-7555496c4c-fq9sm/logs.txt
  19 ./kind-3.kubeconfig/wcm-system/vl3-nse-green-7555496c4c-25rm4/logs.txt
  14 ./kind-1.kubeconfig/wcm-system/connectivity-domain-operator-7cb485f58d-fz4tc/logs.txt
  12 ./kind-1.kubeconfig/spire/spire-agent-zpns4/logs.txt
  10 ./kind-2.kubeconfig/nsm-system/wcm-nsmgr-mxxdf/logs.txt
  10 ./kind-2.kubeconfig/kube-system/kube-apiserver-kind-2-control-plane/logs.txt
  9 ./kind-1.kubeconfig/spire/spire-server-0/logs.txt
  8 ./kind-3.kubeconfig/nsm-system/wcm-nsmgr-45wvw/logs.txt
  7 ./kind-3.kubeconfig/kube-system/kube-apiserver-kind-3-control-plane/logs.txt
  6 ./kind-3.kubeconfig/spire/wcm-spire-server-0/logs.txt
  6 ./kind-2.kubeconfig/spire/wcm-spire-server-0/logs.txt
  5 ./kind-3.kubeconfig/wcm-system/wcm-nse-operator-6fb5b6fb4c-8sbbd/logs.txt
  3 ./kind-2.kubeconfig/kube-system/kube-controller-manager-kind-2-control-plane/logs.txt
  3 ./kind-1.kubeconfig/kube-system/kube-apiserver-kind-1-control-plane/logs.txt
  2 ./kind-3.kubeconfig/nsm-system/wcm-nsm-vpp-forwarder-kcstt/logs.txt
  2 ./kind-3.kubeconfig/kube-system/kube-controller-manager-kind-3-control-plane/logs.txt
  2 ./kind-3.kubeconfig/kube-system/coredns-6955765f44-xktmf/logs.txt
  2 ./kind-3.kubeconfig/kube-system/coredns-6955765f44-g7ndn/logs.txt
  2 ./kind-2.kubeconfig/wcm-system/wcm-nse-operator-6fb5b6fb4c-28plh/logs.txt
  2 ./kind-2.kubeconfig/nsm-system/wcm-nsm-vpp-forwarder-pjlqx/logs.txt
  2 ./kind-2.kubeconfig/kube-system/coredns-6955765f44-jl66h/logs.txt
  2 ./kind-2.kubeconfig/kube-system/coredns-6955765f44-9w8f5/logs.txt
  2 ./kind-1.kubeconfig/wcm-system/wcm-etcd-cluster-fhx9v28lsz/logs.txt
  2 ./kind-1.kubeconfig/wcm-system/external-dns-849696cb6d-sqx86/logs.txt
  2 ./kind-1.kubeconfig/wcm-system/etcd-operator-84cf6bc5d5-j9zbv/logs.txt
  2 ./kind-1.kubeconfig/istio-system/istiod-85c587c85c-htzcm/logs.txt
  2 ./kind-1.kubeconfig/istio-system/istio-ingressgateway-5c658595cb-ps4ls/logs.txt
  1 ./kind-3.kubeconfig/nsm-system/wcm-jaeger-86c548cd44-vns87/logs.txt
  1 ./kind-3.kubeconfig/nsm-system/wcm-crossconnect-monitor-c4db96966-g9w6z/logs.txt
  1 ./kind-3.kubeconfig/kube-system/kube-scheduler-kind-3-control-plane/logs.txt
  1 ./kind-3.kubeconfig/kube-system/kindnet-2wl69/logs.txt
  1 ./kind-2.kubeconfig/nsm-system/wcm-jaeger-86c548cd44-stbrz/logs.txt
  1 ./kind-2.kubeconfig/nsm-system/wcm-crossconnect-monitor-c4db96966-9gpp7/logs.txt
  1 ./kind-2.kubeconfig/kube-system/kube-scheduler-kind-2-control-plane/logs.txt
  1 ./kind-2.kubeconfig/kube-system/kindnet-729mv/logs.txt
  1 ./kind-1.kubeconfig/metallb-system/controller-65895b47d4-8dbkg/logs.txt
  1 ./kind-1.kubeconfig/kube-system/kube-scheduler-kind-1-control-plane/logs.txt
  1 ./kind-1.kubeconfig/kube-system/kindnet-rmtbp/logs.txt

ERRORS OCCURENCES

  ./kind-3.kubeconfig/default/member-core-operator-d598d4888-p7h58/logs.txt
  
  	25:{"level":"error","ts":1623774176.3739984,"logger":"controller_membercore","msg":"An error occurred","component":"Spire Bundle","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:239\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	26:{"level":"error","ts":1623774176.3743465,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	27:{"level":"error","ts":1623774176.3745422,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	261:{"level":"error","ts":1623774194.605516,"logger":"controller_membercore","msg":"An error occurred","component":"NSM common","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:406\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	262:{"level":"error","ts":1623774194.605636,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	263:{"level":"error","ts":1623774194.6057055,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	364:{"level":"error","ts":1623774195.8766756,"logger":"controller_membercore","msg":"An error occurred","component":"NSM common","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:452\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	365:{"level":"error","ts":1623774195.8771193,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	366:{"level":"error","ts":1623774195.8773427,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	467:{"level":"error","ts":1623774198.4979424,"logger":"controller_membercore","msg":"An error occurred","component":"NSM admission Webhook","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:594\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	468:{"level":"error","ts":1623774198.4980767,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	469:{"level":"error","ts":1623774198.4981587,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	529:{"level":"error","ts":1623774199.8818002,"logger":"controller_membercore","msg":"An error occurred","component":"NSM admission Webhook","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:623\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	530:{"level":"error","ts":1623774199.8819225,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	531:{"level":"error","ts":1623774199.8819904,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	707:{"level":"error","ts":1623774203.1508117,"logger":"controller_membercore","msg":"An error occurred","component":"NSManager","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:696\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	708:{"level":"error","ts":1623774203.1509616,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	709:{"level":"error","ts":1623774203.151029,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	975:{"level":"error","ts":1623774208.4554162,"logger":"controller_membercore","msg":"An error occurred","component":"NSE discovery","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:796\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	976:{"level":"error","ts":1623774208.4555497,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	977:{"level":"error","ts":1623774208.455617,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	3439:{"level":"error","ts":1623774718.4752984,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	3440:{"level":"error","ts":1623774718.4756217,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  
  
  ./kind-3.kubeconfig/nsm-system/wcm-crossconnect-monitor-c4db96966-g9w6z/logs.txt
  
  	35:time="2021-06-15T16:23:18Z" level=error msg="failed to read PROMETHEUS env var"
  
  
  ./kind-3.kubeconfig/nsm-system/wcm-jaeger-86c548cd44-vns87/logs.txt
  
  	16:{"level":"info","ts":1623774198.8375766,"caller":"grpc/clientconn.go:1139","msg":"grpc: addrConn.createTransport failed to connect to {127.0.0.1:14250 0  <nil>}. Err :connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:14250: connect: connection refused\". Reconnecting...","system":"grpc","grpc_log":true}
  
  
  ./kind-3.kubeconfig/nsm-system/wcm-nsmgr-45wvw/logs.txt
  
  	1809:time="2021-06-15T16:26:57Z" level=error msg="Cannot forward NSE Registration: rpc error: code = Unknown desc = NSMRS Address variable was not set" operation=nsmgr.RegisterNSE span="4382b3c08e4546b1:07be6c8bea832070:02e1ae329e49e06b:1"
  	1861:time="2021-06-15T16:26:58Z" level=error msg="Cannot forward NSE Registration: rpc error: code = Unknown desc = NSMRS Address variable was not set" operation=nsmgr.RegisterNSE span="0e2f2c049482c52a:4e7ee3de86f11d75:4d90cd1bb0c40355:1"
  	2099:time="2021-06-15T16:28:57Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="2df507d13272ce0c:798e5280b81a80af:1f1b6c9da9885aeb:1"
  	2141:time="2021-06-15T16:28:58Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="13576f43a9655788:3b79a091a0b74846:2c8dbf3abbd2a934:1"
  	2429:time="2021-06-15T16:30:57Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="2df507d13272ce0c:798e5280b81a80af:1f1b6c9da9885aeb:1"
  	2471:time="2021-06-15T16:30:58Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="13576f43a9655788:3b79a091a0b74846:2c8dbf3abbd2a934:1"
  	2749:time="2021-06-15T16:32:57Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="2df507d13272ce0c:798e5280b81a80af:1f1b6c9da9885aeb:1"
  	2791:time="2021-06-15T16:32:58Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="13576f43a9655788:3b79a091a0b74846:2c8dbf3abbd2a934:1"
  
  
  ./kind-3.kubeconfig/nsm-system/wcm-nsm-vpp-forwarder-kcstt/logs.txt
  
  	122:time="2021-06-15 16:23:23.85242" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: read tcp 127.0.0.1:9111->127.0.0.1:46140: read: connection reset by peer\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	149:time="2021-06-15 16:23:24.01041" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  
  
  ./kind-3.kubeconfig/spire/wcm-spire-server-0/logs.txt
  
  	64:time="2021-06-15T16:23:05Z" level=error msg="Error updating bundle" error="local bundle not found" subsystem_name=bundle_client trust_domain=central.com
  	77:time="2021-06-15T16:23:50Z" level=error msg="Bundle not found" method=fetch_federated_bundle subsystem_name=registration_api
  	78:time="2021-06-15T16:23:50Z" level=error msg="Bundle not found" method=fetch_federated_bundle subsystem_name=registration_api
  	82:time="2021-06-15T16:24:48Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  	85:time="2021-06-15T16:30:29Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  	87:time="2021-06-15T16:30:30Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  
  
  ./kind-3.kubeconfig/spire/wcm-spire-agent-bj8b2/logs.txt
  
  	183:time="2021-06-15T16:23:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	195:time="2021-06-15T16:24:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	270:time="2021-06-15T16:25:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	298:time="2021-06-15T16:26:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	415:time="2021-06-15T16:27:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	475:time="2021-06-15T16:28:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	535:time="2021-06-15T16:29:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	566:time="2021-06-15T16:30:20Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	567:time="2021-06-15T16:30:20Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	568:time="2021-06-15T16:30:20Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	570:time="2021-06-15T16:30:20Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5202 subsystem_name=workload_api
  	574:time="2021-06-15T16:30:20Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	577:time="2021-06-15T16:30:20Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5242 subsystem_name=workload_api
  	580:time="2021-06-15T16:30:20Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	582:time="2021-06-15T16:30:20Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	593:time="2021-06-15T16:30:21Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	596:time="2021-06-15T16:30:21Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	601:time="2021-06-15T16:30:21Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5242 subsystem_name=workload_api
  	606:time="2021-06-15T16:30:21Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5202 subsystem_name=workload_api
  	609:time="2021-06-15T16:30:21Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	612:time="2021-06-15T16:30:21Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	615:time="2021-06-15T16:30:21Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	618:time="2021-06-15T16:30:21Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	628:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	633:time="2021-06-15T16:30:23Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5242 subsystem_name=workload_api
  	636:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	639:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	642:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	648:time="2021-06-15T16:30:23Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5202 subsystem_name=workload_api
  	651:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	654:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	667:time="2021-06-15T16:30:26Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5242 subsystem_name=workload_api
  	670:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	673:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	676:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	679:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	685:time="2021-06-15T16:30:26Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5202 subsystem_name=workload_api
  	687:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	690:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	706:time="2021-06-15T16:30:28Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	716:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	720:time="2021-06-15T16:30:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	831:time="2021-06-15T16:31:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	921:time="2021-06-15T16:32:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	981:time="2021-06-15T16:33:30Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  
  
  ./kind-3.kubeconfig/wcm-system/vl3-nse-green-7555496c4c-25rm4/logs.txt
  
  	50:time="2021-06-15 16:25:27.38920" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	100:time="2021-06-15T16:25:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	102:time="2021-06-15T16:25:57Z" level=error msg="Error occurred while trying to establish connection to: wcmd-green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	257:time="2021-06-15 16:27:28.48787" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	388:time="2021-06-15T16:27:57Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	413:time="2021-06-15T16:28:27Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	439:time="2021-06-15T16:28:57Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	463:time="2021-06-15T16:29:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	479:time="2021-06-15T16:29:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	493:time="2021-06-15T16:30:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	507:time="2021-06-15T16:30:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = error reading from server: EOF"
  	521:time="2021-06-15T16:30:57Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	535:time="2021-06-15T16:31:27Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	549:time="2021-06-15T16:31:57Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	563:time="2021-06-15T16:32:27Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	577:time="2021-06-15T16:32:57Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	591:time="2021-06-15T16:33:27Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	605:time="2021-06-15T16:33:57Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	619:time="2021-06-15T16:34:27Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  
  
  ./kind-3.kubeconfig/wcm-system/wcm-nse-discovery-5f68d85698-2xkfn/logs.txt
  
  	103:time="2021-06-15T16:27:07Z" level=error msg="None of the tls certificates worked: <nil>"
  	105:time="2021-06-15T16:27:27Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	106:time="2021-06-15T16:27:27Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="747f8414fffea5f2:1f8006767159ec34:747f8414fffea5f2:1"
  	109:{"level":"info","ts":1623774447.8460631,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	159:time="2021-06-15T16:27:37Z" level=error msg="None of the tls certificates worked: <nil>"
  	161:time="2021-06-15T16:27:57Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	162:time="2021-06-15T16:27:57Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="14e0cb5dcd26f184:56985baa73a3d75d:14e0cb5dcd26f184:1"
  	165:{"level":"info","ts":1623774477.8915133,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	213:time="2021-06-15T16:28:07Z" level=error msg="None of the tls certificates worked: <nil>"
  	215:time="2021-06-15T16:28:27Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	216:time="2021-06-15T16:28:27Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="652fca6d847f8a44:380aaab2aa254fa5:652fca6d847f8a44:1"
  	219:{"level":"info","ts":1623774507.9426143,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	267:time="2021-06-15T16:28:38Z" level=error msg="None of the tls certificates worked: <nil>"
  	269:time="2021-06-15T16:28:58Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	270:time="2021-06-15T16:28:58Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="3883de219c36da50:228a6588a58e07ab:3883de219c36da50:1"
  	273:{"level":"info","ts":1623774538.0095677,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	321:time="2021-06-15T16:29:08Z" level=error msg="None of the tls certificates worked: <nil>"
  	323:time="2021-06-15T16:29:28Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	324:time="2021-06-15T16:29:28Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="36f0855ad5cc13e4:226b1bef2b96e19d:36f0855ad5cc13e4:1"
  	327:{"level":"info","ts":1623774568.0658405,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	377:time="2021-06-15T16:29:38Z" level=error msg="None of the tls certificates worked: <nil>"
  	379:time="2021-06-15T16:29:58Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	380:time="2021-06-15T16:29:58Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="2c041b295577d2b0:0753ed4e80777cc2:2c041b295577d2b0:1"
  	383:{"level":"info","ts":1623774598.1149209,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	431:time="2021-06-15T16:30:08Z" level=error msg="None of the tls certificates worked: <nil>"
  	433:time="2021-06-15T16:30:28Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	434:time="2021-06-15T16:30:28Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="7a25691e90a68507:6091d1b228bc77b9:7a25691e90a68507:1"
  	437:{"level":"info","ts":1623774628.1742108,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	485:time="2021-06-15T16:30:38Z" level=error msg="None of the tls certificates worked: <nil>"
  	487:time="2021-06-15T16:30:58Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	488:time="2021-06-15T16:30:58Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="3c57257e4f79a6d6:1d0f2bfac80e4c3c:3c57257e4f79a6d6:1"
  	491:{"level":"info","ts":1623774658.2427907,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	539:time="2021-06-15T16:31:08Z" level=error msg="None of the tls certificates worked: <nil>"
  	541:time="2021-06-15T16:31:28Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	542:time="2021-06-15T16:31:28Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="3b31b8a22dbe7bf0:7278266b1b16a6b0:3b31b8a22dbe7bf0:1"
  	545:{"level":"info","ts":1623774688.2925444,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	595:time="2021-06-15T16:31:38Z" level=error msg="None of the tls certificates worked: <nil>"
  	597:time="2021-06-15T16:31:58Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	598:time="2021-06-15T16:31:58Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="4004d37641e39d58:2d7b3a3fe91fd2c3:4004d37641e39d58:1"
  	601:{"level":"info","ts":1623774718.3368409,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	649:time="2021-06-15T16:32:10Z" level=error msg="None of the tls certificates worked: <nil>"
  	651:time="2021-06-15T16:32:30Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	652:time="2021-06-15T16:32:30Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="50fddfb8458597a0:5219a4d5e6224d9e:50fddfb8458597a0:1"
  	655:{"level":"info","ts":1623774750.8910108,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	703:time="2021-06-15T16:32:40Z" level=error msg="None of the tls certificates worked: <nil>"
  	705:time="2021-06-15T16:33:00Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	706:time="2021-06-15T16:33:00Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="3def4b358ce3125a:22bb96613f8dcb8d:3def4b358ce3125a:1"
  	709:{"level":"info","ts":1623774780.931901,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	757:time="2021-06-15T16:33:10Z" level=error msg="None of the tls certificates worked: <nil>"
  	761:time="2021-06-15T16:33:30Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	762:time="2021-06-15T16:33:30Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="050126a746d70351:0312fd80bee1785f:050126a746d70351:1"
  	765:{"level":"info","ts":1623774810.9665675,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green2cw8m green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green2cw8m e166d1c1-c82e-48c0-94c6-98efe731b78b 3823 1 2021-06-15 16:26:57 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-25rm4 wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	813:time="2021-06-15T16:33:41Z" level=error msg="None of the tls certificates worked: <nil>"
  	815:time="2021-06-15T16:34:01Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	816:time="2021-06-15T16:34:01Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="245e52c74c120745:31224c8a57dd1de8:245e52c74c120745:1"
  	819:{"level":"info","ts":1623774841.0004656,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greent9mhz green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greent9mhz 7bd8a158-3bc1-4173-b0cd-55c6067f6c24 3828 1 2021-06-15 16:26:58 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-3_vl3-nse-green-7555496c4c-fq9sm wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-3-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	867:time="2021-06-15T16:34:11Z" level=error msg="None of the tls certificates worked: <nil>"
  
  
  ./kind-3.kubeconfig/wcm-system/vl3-nse-green-7555496c4c-fq9sm/logs.txt
  
  	52:time="2021-06-15 16:25:27.52784" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	100:time="2021-06-15T16:25:58Z" level=error msg="None of the tls certificates worked: <nil>"
  	102:time="2021-06-15T16:25:58Z" level=error msg="Error occurred while trying to establish connection to: wcmd-green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	236:time="2021-06-15 16:27:28.55190" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	343:time="2021-06-15T16:27:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	368:time="2021-06-15T16:28:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	394:time="2021-06-15T16:28:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	418:time="2021-06-15T16:29:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	434:time="2021-06-15T16:29:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	448:time="2021-06-15T16:30:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = error reading from server: EOF"
  	462:time="2021-06-15T16:30:31Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	476:time="2021-06-15T16:30:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	490:time="2021-06-15T16:31:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	504:time="2021-06-15T16:32:00Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	518:time="2021-06-15T16:32:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	532:time="2021-06-15T16:32:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	546:time="2021-06-15T16:33:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	560:time="2021-06-15T16:33:58Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	574:time="2021-06-15T16:34:28Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  
  
  ./kind-3.kubeconfig/wcm-system/wcm-nse-operator-6fb5b6fb4c-8sbbd/logs.txt
  
  	13:{"level":"info","ts":1623774178.1689165,"logger":"cmd","msg":"Could not create ServiceMonitor object","error":"no ServiceMonitor registered with the API"}
  	14:{"level":"info","ts":1623774178.1693769,"logger":"cmd","msg":"Install prometheus-operator in your cluster to create ServiceMonitor objects","error":"no ServiceMonitor registered with the API"}
  	52:{"level":"error","ts":1623774325.885153,"logger":"controller_connectivitydomainendpoint","msg":"An error occurred","component":"NSE","method":"UpdateObject","error":"Operation cannot be fulfilled on connectivitydomainendpoints.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/wcm-nse-operator/pkg/controller/connectivitydomainendpoint.(*ReconcileConnectivityDomainEndpoint).Reconcile\n\twcm-nse-operator/pkg/controller/connectivitydomainendpoint/connectivitydomainendpoint_controller.go:229\ncisco-app-networking.github.io/wcm-api/wcm-nse-operator/pkg/controller/connectivitydomainendpoint.(*LoggingConnectivityDomainEndpointReconciler).Reconcile\n\twcm-nse-operator/pkg/controller/connectivitydomainendpoint/connectivitydomainendpoint_controller.go:102\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	53:{"level":"error","ts":1623774325.8858345,"logger":"controller_connectivitydomainendpoint","msg":"An Error occurred","method":"Reconcile","operator":"connectivity domain endpoint","request namespace":"wcm-system","request name":"green","error":"Operation cannot be fulfilled on connectivitydomainendpoints.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/wcm-nse-operator/pkg/controller/connectivitydomainendpoint.(*LoggingConnectivityDomainEndpointReconciler).Reconcile.func1\n\twcm-nse-operator/pkg/controller/connectivitydomainendpoint/connectivitydomainendpoint_controller.go:91\ncisco-app-networking.github.io/wcm-api/wcm-nse-operator/pkg/controller/connectivitydomainendpoint.(*LoggingConnectivityDomainEndpointReconciler).Reconcile\n\twcm-nse-operator/pkg/controller/connectivitydomainendpoint/connectivitydomainendpoint_controller.go:102\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	54:{"level":"error","ts":1623774325.886015,"logger":"controller","msg":"Reconciler error","controller":"connectivitydomainendpoint-controller","name":"green","namespace":"wcm-system","error":"Operation cannot be fulfilled on connectivitydomainendpoints.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  
  
  ./kind-3.kubeconfig/kube-system/kube-apiserver-kind-3-control-plane/logs.txt
  
  	275:E0615 16:24:48.337562       1 upgradeaware.go:371] Error proxying data from backend to client: write tcp 172.17.0.4:6443->172.17.0.1:51312: write: broken pipe
  	283:E0615 16:30:26.691931       1 upgradeaware.go:371] Error proxying data from backend to client: tls: use of closed connection
  	286:E0615 16:32:07.173563       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.4:46536->172.17.0.4:10250: write: broken pipe
  	287:E0615 16:32:09.100586       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.4:46756->172.17.0.4:10250: write: broken pipe
  	288:E0615 16:32:09.199704       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.4:46766->172.17.0.4:10250: write: broken pipe
  	289:E0615 16:32:09.200203       1 upgradeaware.go:371] Error proxying data from backend to client: tls: use of closed connection
  	290:E0615 16:32:11.199815       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.4:46988->172.17.0.4:10250: write: broken pipe
  
  
  ./kind-3.kubeconfig/kube-system/kube-scheduler-kind-3-control-plane/logs.txt
  
  	6:W0615 16:09:51.759648       1 authentication.go:296] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
  
  
  ./kind-3.kubeconfig/kube-system/kindnet-2wl69/logs.txt
  
  	4:I0615 16:10:44.535307       1 main.go:104] Failed to get nodes, retrying after error: Get https://10.96.0.1:443/api/v1/nodes: dial tcp 10.96.0.1:443: i/o timeout
  
  
  ./kind-3.kubeconfig/kube-system/kube-controller-manager-kind-3-control-plane/logs.txt
  
  	10:E0615 16:09:51.754010       1 leaderelection.go:331] error retrieving resource lock kube-system/kube-controller-manager: endpoints "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "endpoints" in API group "" in the namespace "kube-system"
  	219:E0615 16:23:25.196152       1 daemon_controller.go:290] nsm-system/wcm-nsmgr failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"wcm-nsmgr", GenerateName:"", Namespace:"nsm-system", SelfLink:"/apis/apps/v1/namespaces/nsm-system/daemonsets/wcm-nsmgr", UID:"82703f6a-7073-4d87-84d4-250d32051b00", ResourceVersion:"2946", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759371004, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"wcm-nsmgr", "membercore":"wcm"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference{v1.OwnerReference{APIVersion:"wcm.cisco.com/v1alpha1", Kind:"MemberCore", Name:"wcm", UID:"a917afac-4220-4d43-b627-3378e4a6abab", Controller:(*bool)(0xc001064b8d), BlockOwnerDeletion:(*bool)(0xc001064b8e)}}, Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc000648140), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"wcm-nsmgr", "membercore":"wcm"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kubelet-socket", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc000648180), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"nsm-socket", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc0006481a0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"nsm-config-volume", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc000fe6940), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"spire-agent-socket", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc0006481c0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"wcm-nsmgr", Image:"nishantapatil3/nsmdp:vl3_latest", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort{v1.ContainerPort{Name:"", HostPort:5001, ContainerPort:5001, Protocol:"TCP", HostIP:""}}, EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"INSECURE", Value:"false", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"TRACER_ENABLED", Value:"true", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"JAEGER_AGENT_HOST", Value:"wcm-jaeger.nsm-system", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"JAEGER_AGENT_PORT", Value:"6831", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PROXY_NSMD_K8S_ADDRESS", Value:"wcm-proxy-nsmgr:5005", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PROXY_NSMD_ADDRESS", Value:"wcm-proxy-nsmgr:5006", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"nsm-socket", ReadOnly:false, MountPath:"/var/lib/networkservicemesh", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"kubelet-socket", ReadOnly:false, MountPath:"/var/lib/kubelet/device-plugins", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"spire-agent-socket", ReadOnly:true, MountPath:"/run/spire/sockets", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"nsmd", Image:"nishantapatil3/nsmd:vl3_latest", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"INSECURE", Value:"false", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"TRACER_ENABLED", Value:"true", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"JAEGER_AGENT_HOST", Value:"wcm-jaeger.nsm-system", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"JAEGER_AGENT_PORT", Value:"6831", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PROXY_NSMD_K8S_ADDRESS", Value:"wcm-proxy-nsmgr:5005", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PROXY_NSMD_ADDRESS", Value:"wcm-proxy-nsmgr:5006", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PREFERRED_REMOTE_MECHANISM", Value:"IPSEC", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"nsm-socket", ReadOnly:false, MountPath:"/var/lib/networkservicemesh", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"nsm-config-volume", ReadOnly:false, MountPath:"/var/lib/networkservicemesh/config", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"spire-agent-socket", ReadOnly:true, MountPath:"/run/spire/sockets", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(0xc000c1ce40), ReadinessProbe:(*v1.Probe)(0xc000c1ce70), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"nsmd-k8s", Image:"nishantapatil3/nsmd-k8s:vl3_latest", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"INSECURE", Value:"false", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"POD_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc0006481e0)}, v1.EnvVar{Name:"POD_UID", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc000648240)}, v1.EnvVar{Name:"NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc000648280)}, v1.EnvVar{Name:"TRACER_ENABLED", Value:"true", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"JAEGER_AGENT_HOST", Value:"wcm-jaeger.nsm-system", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"JAEGER_AGENT_PORT", Value:"6831", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PROXY_NSMD_K8S_ADDRESS", Value:"wcm-proxy-nsmgr:5005", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PROXY_NSMD_ADDRESS", Value:"wcm-proxy-nsmgr:5006", ValueFrom:(*v1.EnvVarSource)(nil)}, v1.EnvVar{Name:"PREFERRED_REMOTE_MECHANISM", Value:"IPSEC", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"spire-agent-socket", ReadOnly:true, MountPath:"/run/spire/sockets", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001064d98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"wcm-nsmgr-service-account", DeprecatedServiceAccount:"wcm-nsmgr-service-account", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000ef88a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc0018da820)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc001064dac)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "wcm-nsmgr": the object has been modified; please apply your changes to the latest version and try again
  
  
  ./kind-3.kubeconfig/kube-system/coredns-6955765f44-g7ndn/logs.txt
  
  	12:[ERROR] Restart failed: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  	13:[ERROR] plugin/reload: Corefile changed but reload failed: starting with listener file descriptors: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  
  
  ./kind-3.kubeconfig/kube-system/coredns-6955765f44-xktmf/logs.txt
  
  	12:[ERROR] Restart failed: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  	13:[ERROR] plugin/reload: Corefile changed but reload failed: starting with listener file descriptors: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  
  
  ./kind-1.kubeconfig/spire/spire-server-0/logs.txt
  
  	45:time="2021-06-15T16:22:29Z" level=error msg="Error updating bundle" error="local bundle not found" subsystem_name=bundle_client trust_domain=member1.com
  	47:time="2021-06-15T16:22:29Z" level=error msg="Error updating bundle" error="local bundle not found" subsystem_name=bundle_client trust_domain=member2.com
  	57:time="2021-06-15T16:23:29Z" level=error msg="Error updating bundle" error="local bundle not found" subsystem_name=bundle_client trust_domain=member1.com
  	59:time="2021-06-15T16:23:29Z" level=error msg="Error updating bundle" error="local bundle not found" subsystem_name=bundle_client trust_domain=member2.com
  	61:time="2021-06-15T16:23:49Z" level=error msg="Bundle not found" method=fetch_federated_bundle subsystem_name=registration_api
  	62:time="2021-06-15T16:23:49Z" level=error msg="Bundle not found" method=fetch_federated_bundle subsystem_name=registration_api
  	72:time="2021-06-15T16:30:28Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  	73:time="2021-06-15T16:30:28Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  	74:time="2021-06-15T16:30:28Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  
  
  ./kind-1.kubeconfig/spire/spire-agent-zpns4/logs.txt
  
  	26:time="2021-06-15T16:23:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	30:time="2021-06-15T16:24:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	86:time="2021-06-15T16:25:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	100:time="2021-06-15T16:26:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	104:time="2021-06-15T16:27:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	108:time="2021-06-15T16:28:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	112:time="2021-06-15T16:29:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	116:time="2021-06-15T16:30:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	133:time="2021-06-15T16:31:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	137:time="2021-06-15T16:32:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	141:time="2021-06-15T16:33:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	145:time="2021-06-15T16:34:14Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  
  
  ./kind-1.kubeconfig/metallb-system/controller-65895b47d4-8dbkg/logs.txt
  
  	19:{"caller":"service.go:85","error":"controller not synced","msg":"controller not synced yet, cannot allocate IP; will retry after sync","op":"allocateIP","service":"spire/spire-server","ts":"2021-06-15T16:22:36.2250385Z"}
  
  
  ./kind-1.kubeconfig/wcm-system/green-bfd874d66-4s557/logs.txt
  
  	10:2021/06/15 16:24:56 ERROR: Reference contains invalid type of SpanReference: {}
  	12:2021/06/15 16:24:56 ERROR: Reference contains invalid type of SpanReference: {}
  	27:2021/06/15 16:24:57 ERROR: Reference contains invalid type of SpanReference: {}
  	37:time="2021-06-15T16:30:31Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="69d7fcac48ad33e2:3c7d6cae94ec8115:5dcafb7ba32d6056:1"
  	44:time="2021-06-15T16:30:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="133b7bccf3e49890:74e2c1084630be7b:7425beadb19751d6:1"
  	51:time="2021-06-15T16:30:48Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="27d7f6244ad6b4e9:7d56802d2f0da080:588060e2c598d166:1"
  	58:time="2021-06-15T16:30:57Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="1bfbbb0c21cddbc5:0a37d832f9cfc692:23a9d970f8192a48:1"
  	65:time="2021-06-15T16:30:58Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="0121f374ffbf0c03:68c30e06c4705e46:5026beef7146b57a:1"
  	72:time="2021-06-15T16:31:17Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="3f312cd03604f987:12171bcc3417850c:4a5a33cf996c180b:1"
  	79:time="2021-06-15T16:31:17Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="41efa72d3a4e8311:3533a827b2c10744:58ce94bac95eb8ee:1"
  	86:time="2021-06-15T16:31:27Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="4d8ff8b472742a6c:6852a14308788989:1b1a2b2abb218b9d:1"
  	93:time="2021-06-15T16:31:28Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="0393de04dec71248:7d793c51c2bcdbb2:6bd6b3768fd067ec:1"
  	100:time="2021-06-15T16:31:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="3265ad06af3cf756:5bc5834b433ef0af:2c70d4e7912f205c:1"
  	107:time="2021-06-15T16:31:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="4d0462b95bad38c5:06dad2558a090ceb:28ad15fc74a9a67e:1"
  	114:time="2021-06-15T16:31:57Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="0ddcbd796acf8d67:707f8490dcd382bb:3b7f21b3369cb58b:1"
  	121:time="2021-06-15T16:32:00Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="0e707351039af9c7:7344b1e6c1a49433:0070207100187ea1:1"
  	128:time="2021-06-15T16:32:17Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="3cac2569098b8ab1:35d16f7117e35331:6e78880380b01117:1"
  	135:time="2021-06-15T16:32:17Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="2ac024b599022254:2f3960cd59bb4e3b:54103b7ca11779cf:1"
  	142:time="2021-06-15T16:32:27Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="2606d133f57d7cac:7507e36261bef958:0f8c17543cade5cd:1"
  	149:time="2021-06-15T16:32:28Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="442dcf99473d173e:40f150ba6abf9285:2378b8ef01a9adb0:1"
  	156:time="2021-06-15T16:32:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="038e8534882572a2:6985d6a2258a5a9f:7a5356cfc468c987:1"
  	163:time="2021-06-15T16:32:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="28369f3ebf7ced24:5b9048f01e2a6066:638e2f777bdf2693:1"
  	170:time="2021-06-15T16:32:57Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="059e2f228448a5b0:0b65da4ff8e49b2a:5a1c47e57afda4be:1"
  	177:time="2021-06-15T16:32:58Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="66939d71b1558c84:4d14a40091577e91:0deb110a9fa93cd4:1"
  	184:time="2021-06-15T16:33:17Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="0664258d48d1acaf:4e8969271c7c26ec:4d4d4a211cf5cd43:1"
  	191:time="2021-06-15T16:33:17Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="7131d6d959fbcce2:5c0b00c37032f42d:6cea67c5cfc05023:1"
  	198:time="2021-06-15T16:33:27Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="2cea60136e2b6d19:38b2261bc3ba4e55:09ea978537efef27:1"
  	205:time="2021-06-15T16:33:28Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="1a26df9464ccec04:3cb5aa2a0506321a:67fe9c6f727e3982:1"
  	212:time="2021-06-15T16:33:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="6a3a8f03309bf359:659d395cb9af9047:5c88b39985456d77:1"
  	219:time="2021-06-15T16:33:47Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="255ddf3d802a836a:1c535f07423eaa93:72d9f1eb44f61b21:1"
  	226:time="2021-06-15T16:33:57Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="355851c533043593:398e2d7928140b29:61e9a63effb85444:1"
  	233:time="2021-06-15T16:33:58Z" level=error msg="Cannot find Network Service: rpc error: code = NotFound desc = no NetworkService with name: green" operation=Nsmrs.FindNetworkService span="7564356fce694843:0909ac29c79a915e:0fd599d7823bfc14:1"
  
  
  ./kind-1.kubeconfig/wcm-system/connectivity-domain-operator-7cb485f58d-fz4tc/logs.txt
  
  	12:{"level":"info","ts":1623774158.299078,"logger":"cmd","msg":"Could not create metrics Service","error":"failed to initialize service object for metrics: replicasets.apps \"connectivity-domain-operator-7cb485f58d\" is forbidden: User \"system:serviceaccount:wcm-system:connectivity-domain-operator-service-account\" cannot get resource \"replicasets\" in API group \"apps\" in the namespace \"wcm-system\""}
  	13:{"level":"info","ts":1623774158.753427,"logger":"cmd","msg":"Could not create ServiceMonitor object","error":"no ServiceMonitor registered with the API"}
  	14:{"level":"info","ts":1623774158.7536073,"logger":"cmd","msg":"Install prometheus-operator in your cluster to create ServiceMonitor objects","error":"no ServiceMonitor registered with the API"}
  	25:{"level":"error","ts":1623774294.3757787,"logger":"controller_connectivitydomain","msg":"An error occurred","component":"NSR","method":"UpdateObject","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*ReconcileConnectivityDomain).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:236\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	26:{"level":"error","ts":1623774294.3773367,"logger":"controller_connectivitydomain","msg":"An Error occurred","method":"Reconcile","operator":"connectivity domain","request namespace":"wcm-system","request name":"green","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile.func1\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:106\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	27:{"level":"error","ts":1623774294.377483,"logger":"controller","msg":"Reconciler error","controller":"connectivitydomain-controller","name":"green","namespace":"wcm-system","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	56:{"level":"error","ts":1623774295.688437,"logger":"controller_connectivitydomain","msg":"An error occurred","component":"WCMD","method":"UpdateObject","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*ReconcileConnectivityDomain).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:296\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	57:{"level":"error","ts":1623774295.6888087,"logger":"controller_connectivitydomain","msg":"An Error occurred","method":"Reconcile","operator":"connectivity domain","request namespace":"wcm-system","request name":"green","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile.func1\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:106\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	58:{"level":"error","ts":1623774295.689055,"logger":"controller","msg":"Reconciler error","controller":"connectivitydomain-controller","name":"green","namespace":"wcm-system","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	157:{"level":"error","ts":1623774297.6511238,"logger":"controller_connectivitydomain","msg":"An error occurred","component":"istio","method":"UpdateObject","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*ReconcileConnectivityDomain).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:395\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	158:{"level":"error","ts":1623774297.6514792,"logger":"controller_connectivitydomain","msg":"An Error occurred","method":"Reconcile","operator":"connectivity domain","request namespace":"wcm-system","request name":"green","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile.func1\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:106\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	159:{"level":"error","ts":1623774297.651556,"logger":"controller","msg":"Reconciler error","controller":"connectivitydomain-controller","name":"green","namespace":"wcm-system","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	334:{"level":"error","ts":1623774301.4419937,"logger":"controller_connectivitydomain","msg":"An Error occurred","method":"Reconcile","operator":"connectivity domain","request namespace":"wcm-system","request name":"green","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile.func1\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:106\ncisco-app-networking.github.io/wcm-api/connectivity-domain-operator/pkg/controller/connectivitydomain.(*LoggingConnectivityDomainReconciler).Reconcile\n\tconnectivity-domain-operator/pkg/controller/connectivitydomain/connectivitydomain_controller.go:117\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	335:{"level":"error","ts":1623774301.4421027,"logger":"controller","msg":"Reconciler error","controller":"connectivitydomain-controller","name":"green","namespace":"wcm-system","error":"Operation cannot be fulfilled on connectivitydomains.wcm.cisco.com \"green\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  
  
  ./kind-1.kubeconfig/wcm-system/wcm-etcd-cluster-fhx9v28lsz/logs.txt
  
  	189:WARNING: 2021/06/15 16:23:20 grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
  	250:WARNING: 2021/06/15 16:23:40 grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
  
  
  ./kind-1.kubeconfig/wcm-system/etcd-operator-84cf6bc5d5-j9zbv/logs.txt
  
  	30:time="2021-06-15T16:23:37Z" level=error msg="failed to reconcile: fail to add new member (wcm-etcd-cluster-gnswn5jd8d): etcdserver: re-configuration failed due to not enough started members" cluster-name=wcm-etcd-cluster cluster-namespace=wcm-system pkg=cluster
  	35:time="2021-06-15T16:23:45Z" level=error msg="failed to reconcile: fail to add new member (wcm-etcd-cluster-thpt6qgvfh): etcdserver: unhealthy cluster" cluster-name=wcm-etcd-cluster cluster-namespace=wcm-system pkg=cluster
  
  
  ./kind-1.kubeconfig/wcm-system/external-dns-849696cb6d-sqx86/logs.txt
  
  	28:{"level":"warn","ts":"2021-06-15T16:23:21.857Z","caller":"clientv3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"endpoint://client-bd1915bc-a813-4062-8a5f-978e0d5c14ee/wcm-etcd-cluster-client.wcm-system:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
  	29:time="2021-06-15T16:23:21Z" level=error msg="context deadline exceeded"
  
  
  ./kind-1.kubeconfig/kube-system/kube-controller-manager-kind-1-control-plane/logs.txt
  
  	10:E0615 16:08:53.085424       1 leaderelection.go:331] error retrieving resource lock kube-system/kube-controller-manager: endpoints "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "endpoints" in API group "" in the namespace "kube-system"
  	189:E0615 16:09:11.524585       1 daemon_controller.go:290] kube-system/kube-proxy failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy", UID:"07e78203-a656-497a-8161-15ebc355e0f0", ResourceVersion:"215", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759370136, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc00073be40), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-proxy", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc000f7e700), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"xtables-lock", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc00073be60), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"lib-modules", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc00073be80), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"kube-proxy", Image:"k8s.gcr.io/kube-proxy:v1.17.5", Command:[]string{"/usr/local/bin/kube-proxy", "--config=/var/lib/kube-proxy/config.conf", "--hostname-override=$(NODE_NAME)"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc00073bec0)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-proxy", ReadOnly:false, MountPath:"/var/lib/kube-proxy", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"xtables-lock", ReadOnly:false, MountPath:"/run/xtables.lock", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"lib-modules", ReadOnly:true, MountPath:"/lib/modules", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc000e0b400), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00087c388), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"beta.kubernetes.io/os":"linux"}, ServiceAccountName:"kube-proxy", DeprecatedServiceAccount:"kube-proxy", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00130f620), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"CriticalAddonsOnly", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"system-node-critical", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc0000c6a20)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc00087c3e8)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kube-proxy": the object has been modified; please apply your changes to the latest version and try again
  	190:E0615 16:09:11.529867       1 daemon_controller.go:290] kube-system/kindnet failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kindnet", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kindnet", UID:"105ba1bc-4a86-4c99-821c-083638cd1f5b", ResourceVersion:"403", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759370137, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"kindnet", "k8s-app":"kindnet", "tier":"node"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc000d53140), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"kindnet", "k8s-app":"kindnet", "tier":"node"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"cni-cfg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc000d53160), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"xtables-lock", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc000d53180), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"lib-modules", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc000d531a0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"kindnet-cni", Image:"kindest/kindnetd:0.5.4", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"HOST_IP", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc000d531c0)}, v1.EnvVar{Name:"POD_IP", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc000d53200)}, v1.EnvVar{Name:"POD_SUBNET", Value:"10.244.0.0/16", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"50Mi", Format:"BinarySI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"50Mi", Format:"BinarySI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"cni-cfg", ReadOnly:false, MountPath:"/etc/cni/net.d", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"xtables-lock", ReadOnly:false, MountPath:"/run/xtables.lock", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"lib-modules", ReadOnly:true, MountPath:"/lib/modules", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc000f4fea0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001288f38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"kindnet", DeprecatedServiceAccount:"kindnet", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000feb260), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc0011cc230)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc001288f80)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:1, NumberReady:0, ObservedGeneration:1, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:1, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kindnet": the object has been modified; please apply your changes to the latest version and try again
  	197:E0615 16:22:22.048727       1 daemon_controller.go:290] spire/spire-agent failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"spire-agent", GenerateName:"", Namespace:"spire", SelfLink:"/apis/apps/v1/namespaces/spire/daemonsets/spire-agent", UID:"5fde0259-e1f0-4290-b25a-84a13746e2aa", ResourceVersion:"2591", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759370941, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"spire-agent", "app.kubernetes.io/managed-by":"Helm"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1", "meta.helm.sh/release-name":"spire-release", "meta.helm.sh/release-namespace":"spire"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc001c44b00), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"spire", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"spire-agent"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"spire-config", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc001777240), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"spire-bundle", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc001777280), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"spire-agent-socket", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc001c44b20), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init", Image:"gcr.io/spiffe-io/wait-for-it", Command:[]string(nil), Args:[]string{"-t", "30", "spire-server:8081"}, WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount(nil), VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"spire-agent", Image:"gcr.io/spiffe-io/spire-agent:0.11.3", Command:[]string(nil), Args:[]string{"-config", "/run/spire/config/agent.conf"}, WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"spire-config", ReadOnly:true, MountPath:"/run/spire/config", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"spire-agent-socket", ReadOnly:false, MountPath:"/run/spire/sockets", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"spire-bundle", ReadOnly:true, MountPath:"/run/spire/bundle", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(0xc001d21b00), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0014b5978), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirstWithHostNet", NodeSelector:map[string]string(nil), ServiceAccountName:"spire-agent", DeprecatedServiceAccount:"spire-agent", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:true, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001bc7320), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc00024f1f8)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc0014b59c0)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "spire-agent": the object has been modified; please apply your changes to the latest version and try again
  	198:E0615 16:22:22.065661       1 daemon_controller.go:290] spire/spire-agent failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"spire-agent", GenerateName:"", Namespace:"spire", SelfLink:"/apis/apps/v1/namespaces/spire/daemonsets/spire-agent", UID:"5fde0259-e1f0-4290-b25a-84a13746e2aa", ResourceVersion:"2595", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759370941, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"spire-agent", "app.kubernetes.io/managed-by":"Helm"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1", "meta.helm.sh/release-name":"spire-release", "meta.helm.sh/release-namespace":"spire"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc001d4db40), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"spire", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"spire-agent"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"spire-config", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc00192d240), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"spire-bundle", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc00192d280), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"spire-agent-socket", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc001d4db60), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init", Image:"gcr.io/spiffe-io/wait-for-it", Command:[]string(nil), Args:[]string{"-t", "30", "spire-server:8081"}, WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount(nil), VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"spire-agent", Image:"gcr.io/spiffe-io/spire-agent:0.11.3", Command:[]string(nil), Args:[]string{"-config", "/run/spire/config/agent.conf"}, WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"spire-config", ReadOnly:true, MountPath:"/run/spire/config", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"spire-agent-socket", ReadOnly:false, MountPath:"/run/spire/sockets", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"spire-bundle", ReadOnly:true, MountPath:"/run/spire/bundle", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(0xc001d0a780), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0016ec558), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirstWithHostNet", NodeSelector:map[string]string(nil), ServiceAccountName:"spire-agent", DeprecatedServiceAccount:"spire-agent", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:true, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001b98ba0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc00000fc18)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc0016ec5a0)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:1, NumberReady:0, ObservedGeneration:1, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:1, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "spire-agent": the object has been modified; please apply your changes to the latest version and try again
  	224:E0615 16:23:02.160039       1 disruption.go:504] Error syncing PodDisruptionBudget istio-system/istio-ingressgateway, requeuing: Operation cannot be fulfilled on poddisruptionbudgets.policy "istio-ingressgateway": the object has been modified; please apply your changes to the latest version and try again
  	226:E0615 16:23:13.567284       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	227:I0615 16:23:13.567362       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3091", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	229:E0615 16:23:17.245158       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	230:I0615 16:23:17.245606       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3157", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	246:E0615 16:23:29.174816       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	248:I0615 16:23:29.174924       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	249:E0615 16:23:32.248829       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	250:I0615 16:23:32.248953       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	252:E0615 16:23:44.179378       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	254:I0615 16:23:44.179683       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	255:E0615 16:23:47.252997       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	257:I0615 16:23:47.253335       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	259:E0615 16:23:59.787459       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	261:I0615 16:23:59.787651       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	262:E0615 16:24:02.258174       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	264:I0615 16:24:02.258337       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	265:E0615 16:24:14.791867       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	266:I0615 16:24:14.791933       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	268:E0615 16:24:17.262940       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	270:I0615 16:24:17.263181       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	271:E0615 16:24:30.399184       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	273:I0615 16:24:30.399308       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	274:E0615 16:24:32.268191       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	276:I0615 16:24:32.268709       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	277:E0615 16:24:45.403784       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	279:I0615 16:24:45.403991       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	280:E0615 16:24:47.272549       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	282:I0615 16:24:47.272866       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	289:E0615 16:25:01.010220       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	291:I0615 16:25:01.010683       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	294:E0615 16:25:02.285440       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	295:I0615 16:25:02.285999       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	297:E0615 16:25:16.027420       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	299:I0615 16:25:16.027520       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	300:E0615 16:25:17.289979       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	302:I0615 16:25:17.290276       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	303:E0615 16:25:31.634193       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	304:I0615 16:25:31.634257       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	306:E0615 16:25:32.294099       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	308:I0615 16:25:32.294390       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	309:E0615 16:25:46.639641       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	311:I0615 16:25:46.639819       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	312:E0615 16:25:47.298555       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	313:I0615 16:25:47.298625       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	315:E0615 16:26:02.247088       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	317:I0615 16:26:02.247234       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	318:E0615 16:26:02.302781       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	320:I0615 16:26:02.303285       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	321:E0615 16:26:17.252258       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	323:I0615 16:26:17.252721       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	324:E0615 16:26:17.307492       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	326:I0615 16:26:17.307591       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	327:E0615 16:26:32.860533       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	329:I0615 16:26:32.861052       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	330:E0615 16:26:32.866102       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	332:I0615 16:26:32.866191       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	333:E0615 16:26:47.866470       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	335:I0615 16:26:47.866952       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	336:E0615 16:26:47.870740       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	338:I0615 16:26:47.871087       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	339:E0615 16:27:03.473080       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	341:I0615 16:27:03.473625       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	342:E0615 16:27:03.477478       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	344:I0615 16:27:03.477568       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	345:E0615 16:27:18.477909       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	346:I0615 16:27:18.478200       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	348:E0615 16:27:18.482172       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	350:I0615 16:27:18.482283       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	351:E0615 16:27:34.090517       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	353:I0615 16:27:34.091640       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	354:E0615 16:27:34.098159       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	356:I0615 16:27:34.098287       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	357:E0615 16:27:49.096418       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	359:I0615 16:27:49.096837       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	360:E0615 16:27:49.102477       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	362:I0615 16:27:49.102741       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	363:E0615 16:28:04.704462       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	365:I0615 16:28:04.704821       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	366:E0615 16:28:04.708789       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	368:I0615 16:28:04.709110       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	369:E0615 16:28:19.708826       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	371:I0615 16:28:19.709195       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	372:E0615 16:28:19.712775       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	373:I0615 16:28:19.712945       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	375:E0615 16:28:35.317870       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	377:I0615 16:28:35.318263       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	378:E0615 16:28:35.324033       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	380:I0615 16:28:35.324522       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	381:E0615 16:28:50.323130       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	383:I0615 16:28:50.323297       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	384:E0615 16:28:50.328201       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	386:I0615 16:28:50.328324       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	387:E0615 16:29:05.930745       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	388:I0615 16:29:05.930907       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	390:E0615 16:29:05.943160       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	392:I0615 16:29:05.943584       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	393:E0615 16:29:20.936662       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	395:I0615 16:29:20.937507       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	396:E0615 16:29:20.948343       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	397:I0615 16:29:20.948431       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	399:E0615 16:29:36.543951       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	401:I0615 16:29:36.544099       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	402:E0615 16:29:36.548229       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	404:I0615 16:29:36.548444       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	405:E0615 16:29:51.548655       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	407:I0615 16:29:51.549509       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	408:E0615 16:29:51.552518       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	410:I0615 16:29:51.552976       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	411:E0615 16:30:07.156577       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	412:I0615 16:30:07.156870       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	414:E0615 16:30:07.160430       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	416:I0615 16:30:07.160759       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	417:E0615 16:30:22.161920       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	419:I0615 16:30:22.162229       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	420:E0615 16:30:22.165687       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	422:I0615 16:30:22.166061       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	423:E0615 16:30:37.770018       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	425:I0615 16:30:37.770575       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	426:E0615 16:30:37.776234       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	428:I0615 16:30:37.778372       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	429:E0615 16:30:52.775117       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	431:I0615 16:30:52.775351       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	432:E0615 16:30:52.780630       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	434:I0615 16:30:52.780770       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	435:E0615 16:31:08.382417       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	437:I0615 16:31:08.382586       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	438:E0615 16:31:08.387293       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	440:I0615 16:31:08.387768       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	441:E0615 16:31:23.387268       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	442:I0615 16:31:23.387567       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	444:E0615 16:31:23.391564       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	446:I0615 16:31:23.391671       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	447:E0615 16:31:38.996680       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	449:I0615 16:31:38.997125       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	450:E0615 16:31:39.000929       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	452:I0615 16:31:39.001526       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	453:E0615 16:31:54.001783       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	455:I0615 16:31:54.002750       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	456:E0615 16:31:54.005583       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	458:I0615 16:31:54.005674       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	459:E0615 16:32:09.609481       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	461:I0615 16:32:09.610133       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	462:E0615 16:32:09.613100       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	464:I0615 16:32:09.613557       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	465:E0615 16:32:24.614075       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	467:I0615 16:32:24.614249       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	468:E0615 16:32:24.617939       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	470:I0615 16:32:24.618158       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	471:E0615 16:32:40.222215       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	473:I0615 16:32:40.222413       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	474:E0615 16:32:40.226623       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	476:I0615 16:32:40.226749       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	477:E0615 16:32:55.226908       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	479:I0615 16:32:55.227059       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	480:E0615 16:32:55.231309       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	481:I0615 16:32:55.231421       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	483:E0615 16:33:10.833618       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	485:I0615 16:33:10.833813       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	486:E0615 16:33:10.837106       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	488:I0615 16:33:10.837676       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	489:E0615 16:33:26.440334       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	491:I0615 16:33:26.441137       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	492:E0615 16:33:26.444040       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	494:I0615 16:33:26.444111       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	495:E0615 16:33:41.022591       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	497:I0615 16:33:41.022899       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	498:E0615 16:33:41.027974       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	500:I0615 16:33:41.028552       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	501:E0615 16:33:56.624754       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	503:I0615 16:33:56.625181       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	504:E0615 16:33:56.630765       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	506:I0615 16:33:56.631088       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	507:E0615 16:34:11.022179       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istio-ingressgateway: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	508:I0615 16:34:11.022645       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istio-ingressgateway", UID:"88a53045-4038-4085-8d64-e4924c1bf7dc", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3269", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	510:E0615 16:34:11.026206       1 horizontal.go:214] failed to compute desired number of replicas based on listed metrics for Deployment/istio-system/istiod: invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  	511:I0615 16:34:11.026456       1 event.go:281] Event(v1.ObjectReference{Kind:"HorizontalPodAutoscaler", Namespace:"istio-system", Name:"istiod", UID:"44cd24d7-e637-4920-adf4-705cbdaa18c3", APIVersion:"autoscaling/v2beta2", ResourceVersion:"3240", FieldPath:""}): type: 'Warning' reason: 'FailedComputeMetricsReplicas' invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API: the server could not find the requested resource (get pods.metrics.k8s.io)
  
  
  ./kind-1.kubeconfig/kube-system/kube-apiserver-kind-1-control-plane/logs.txt
  
  	201:E0615 16:21:44.040838       1 watcher.go:214] watch chan error: etcdserver: mvcc: required revision has been compacted
  	297:E0615 16:23:49.858440       1 upgradeaware.go:357] Error proxying data from client to backend: tls: use of closed connection
  	319:E0615 16:30:28.338782       1 upgradeaware.go:371] Error proxying data from backend to client: write tcp 172.17.0.2:6443->172.17.0.1:60412: write: broken pipe
  
  
  ./kind-1.kubeconfig/kube-system/kindnet-rmtbp/logs.txt
  
  	4:I0615 16:09:48.226577       1 main.go:104] Failed to get nodes, retrying after error: Get https://10.96.0.1:443/api/v1/nodes: dial tcp 10.96.0.1:443: i/o timeout
  
  
  ./kind-1.kubeconfig/kube-system/kube-scheduler-kind-1-control-plane/logs.txt
  
  	6:W0615 16:08:53.125828       1 authentication.go:296] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
  
  
  ./kind-1.kubeconfig/istio-system/istio-ingressgateway-5c658595cb-ps4ls/logs.txt
  
  	16:2021-06-15T16:23:03.107043Z	info	FLAG: --proxyComponentLogLevel="misc:error"
  	93:2021-06-15T16:23:03.263785Z	info	Envoy command: [-c etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster istio-ingressgateway --service-node router~10.244.0.14~istio-ingressgateway-5c658595cb-ps4ls.istio-system~istio-system.svc.cluster.local --local-address-ip-version v4 --bootstrap-version 3 --log-format-prefix-with-location 0 --log-format %Y-%m-%dT%T.%fZ	%l	envoy %n	%v -l warning --component-log-level misc:error]
  
  
  ./kind-1.kubeconfig/istio-system/istiod-85c587c85c-htzcm/logs.txt
  
  	128:2021-06-15T16:22:59.996371Z	info	pkica	Failed to get secret (error: secrets "istio-ca-secret" not found), will create one
  	253:2021-06-15T16:23:02.519857Z	info	http: TLS handshake error from 10.244.0.1:21515: remote error: tls: bad certificate
  
  
  ./kind-2.kubeconfig/default/member-core-operator-d598d4888-xd9cx/logs.txt
  
  	25:{"level":"error","ts":1623774173.6714509,"logger":"controller_membercore","msg":"An error occurred","component":"Spire Bundle","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:239\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	26:{"level":"error","ts":1623774173.686763,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	27:{"level":"error","ts":1623774173.687106,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	43:{"level":"error","ts":1623774175.3852043,"logger":"controller_membercore","msg":"An error occurred","component":"Spire Server","method":"UpdateObject","error":"serviceaccounts \"wcm-spire-server-service-account\" already exists","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:261\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	44:{"level":"error","ts":1623774175.3856816,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"serviceaccounts \"wcm-spire-server-service-account\" already exists","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	45:{"level":"error","ts":1623774175.3857992,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"serviceaccounts \"wcm-spire-server-service-account\" already exists","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	101:{"level":"error","ts":1623774177.0127444,"logger":"controller_membercore","msg":"An error occurred","component":"Spire Agent","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:322\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	102:{"level":"error","ts":1623774177.0158446,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	103:{"level":"error","ts":1623774177.0161884,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	311:{"level":"error","ts":1623774192.3499575,"logger":"controller_membercore","msg":"An error occurred","component":"NSM common","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:452\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	312:{"level":"error","ts":1623774192.3500514,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	313:{"level":"error","ts":1623774192.3504138,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	414:{"level":"error","ts":1623774193.6403651,"logger":"controller_membercore","msg":"An error occurred","component":"Jaeger","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:518\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	415:{"level":"error","ts":1623774193.6408248,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	416:{"level":"error","ts":1623774193.6409729,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	593:{"level":"error","ts":1623774197.4355698,"logger":"controller_membercore","msg":"An error occurred","component":"NSManager","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:696\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	594:{"level":"error","ts":1623774197.4356916,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	595:{"level":"error","ts":1623774197.435759,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	801:{"level":"error","ts":1623774200.8682122,"logger":"controller_membercore","msg":"An error occurred","component":"NSE discovery","method":"UpdateObject","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject.func1\n\tcommon-operator-resources/resources.go:859\ncisco-app-networking.github.io/wcm-api/common-operator-resources.LogWCMComponent.UpdateObject\n\tcommon-operator-resources/resources.go:869\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*ReconcileMemberCore).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:758\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	802:{"level":"error","ts":1623774200.8683455,"logger":"controller_member_core","msg":"An Error occurred","method":"Reconcile","operator":"member core","request namespace":"","request name":"wcm","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile.func1\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:107\ncisco-app-networking.github.io/wcm-api/member-core-operator/pkg/controller/membercore.(*LoggingMemberCoreReconciler).Reconcile\n\tmember-core-operator/pkg/controller/membercore/membercore_controller.go:118\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:235\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  	803:{"level":"error","ts":1623774200.8684216,"logger":"controller","msg":"Reconciler error","controller":"membercore-controller","name":"wcm","namespace":"","error":"Operation cannot be fulfilled on membercores.wcm.cisco.com \"wcm\": the object has been modified; please apply your changes to the latest version and try again","stacktrace":"github.com/go-logr/zapr.(*zapLogger).Error\n\t/Users/nispatil/go/pkg/mod/github.com/go-logr/zapr@v0.1.1/zapr.go:128\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:237\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:209\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\n\t/Users/nispatil/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.6.2/pkg/internal/controller/controller.go:188\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:155\nk8s.io/apimachinery/pkg/util/wait.BackoffUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:156\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:133\nk8s.io/apimachinery/pkg/util/wait.Until\n\t/Users/nispatil/go/pkg/mod/k8s.io/apimachinery@v0.18.6/pkg/util/wait/wait.go:90"}
  
  
  ./kind-2.kubeconfig/nsm-system/wcm-nsm-vpp-forwarder-pjlqx/logs.txt
  
  	104:time="2021-06-15 16:23:18.44669" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	130:time="2021-06-15 16:23:18.60902" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  
  
  ./kind-2.kubeconfig/nsm-system/wcm-nsmgr-mxxdf/logs.txt
  
  	1811:time="2021-06-15T16:26:17Z" level=error msg="Cannot forward NSE Registration: rpc error: code = Unknown desc = NSMRS Address variable was not set" operation=nsmgr.RegisterNSE span="7b286ec0e1d93c33:7a95bbdec830f144:58530c95c65ff7f2:1"
  	1863:time="2021-06-15T16:26:17Z" level=error msg="Cannot forward NSE Registration: rpc error: code = Unknown desc = NSMRS Address variable was not set" operation=nsmgr.RegisterNSE span="382ed0789460887d:61b3c989d27720c4:419a0e83d60c3eb5:1"
  	2101:time="2021-06-15T16:28:16Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="749f6d5f021f413d:72b7b3470815fd67:1c2aa861dc74d472:1"
  	2143:time="2021-06-15T16:28:17Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="777b7da252faad01:39401ba4de62efef:02dd7721bcd085f0:1"
  	2421:time="2021-06-15T16:30:16Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="749f6d5f021f413d:72b7b3470815fd67:1c2aa861dc74d472:1"
  	2463:time="2021-06-15T16:30:17Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="777b7da252faad01:39401ba4de62efef:02dd7721bcd085f0:1"
  	2751:time="2021-06-15T16:32:16Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="749f6d5f021f413d:72b7b3470815fd67:1c2aa861dc74d472:1"
  	2793:time="2021-06-15T16:32:17Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="777b7da252faad01:39401ba4de62efef:02dd7721bcd085f0:1"
  	3071:time="2021-06-15T16:34:16Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="749f6d5f021f413d:72b7b3470815fd67:1c2aa861dc74d472:1"
  	3113:time="2021-06-15T16:34:17Z" level=warning msg="Error forwarding BulkRegisterNSE request to wcm-proxy-nsmgr:5005 : EOF" operation=ProxyNsmgr.BulkRegisterNSE span="777b7da252faad01:39401ba4de62efef:02dd7721bcd085f0:1"
  
  
  ./kind-2.kubeconfig/nsm-system/wcm-jaeger-86c548cd44-stbrz/logs.txt
  
  	15:{"level":"info","ts":1623774194.8599038,"caller":"grpc/clientconn.go:1139","msg":"grpc: addrConn.createTransport failed to connect to {127.0.0.1:14250 0  <nil>}. Err :connection error: desc = \"transport: Error while dialing dial tcp 127.0.0.1:14250: connect: connection refused\". Reconnecting...","system":"grpc","grpc_log":true}
  
  
  ./kind-2.kubeconfig/nsm-system/wcm-crossconnect-monitor-c4db96966-9gpp7/logs.txt
  
  	35:time="2021-06-15T16:23:14Z" level=error msg="failed to read PROMETHEUS env var"
  
  
  ./kind-2.kubeconfig/spire/wcm-spire-server-0/logs.txt
  
  	61:time="2021-06-15T16:23:05Z" level=error msg="Error updating bundle" error="local bundle not found" subsystem_name=bundle_client trust_domain=central.com
  	77:time="2021-06-15T16:23:49Z" level=error msg="Bundle not found" method=fetch_federated_bundle subsystem_name=registration_api
  	78:time="2021-06-15T16:23:50Z" level=error msg="Bundle not found" method=fetch_federated_bundle subsystem_name=registration_api
  	82:time="2021-06-15T16:24:48Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  	85:time="2021-06-15T16:30:29Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  	86:time="2021-06-15T16:30:30Z" level=error msg="Entry already exists" method=create_registration_entry subsystem_name=registration_api
  
  
  ./kind-2.kubeconfig/spire/wcm-spire-agent-rmhtd/logs.txt
  
  	178:time="2021-06-15T16:23:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	190:time="2021-06-15T16:24:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	264:time="2021-06-15T16:25:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	365:time="2021-06-15T16:26:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	433:time="2021-06-15T16:27:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	493:time="2021-06-15T16:28:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	553:time="2021-06-15T16:29:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	613:time="2021-06-15T16:30:23Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5157 subsystem_name=workload_api
  	615:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	617:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	619:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	621:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	623:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	626:time="2021-06-15T16:30:23Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5215 subsystem_name=workload_api
  	628:time="2021-06-15T16:30:23Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	639:time="2021-06-15T16:30:24Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	644:time="2021-06-15T16:30:24Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5215 subsystem_name=workload_api
  	647:time="2021-06-15T16:30:24Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	650:time="2021-06-15T16:30:24Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	653:time="2021-06-15T16:30:24Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	656:time="2021-06-15T16:30:24Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	659:time="2021-06-15T16:30:24Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	664:time="2021-06-15T16:30:24Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5157 subsystem_name=workload_api
  	674:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	678:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	681:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	685:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	687:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	692:time="2021-06-15T16:30:26Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5215 subsystem_name=workload_api
  	695:time="2021-06-15T16:30:26Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	700:time="2021-06-15T16:30:26Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5157 subsystem_name=workload_api
  	715:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	725:time="2021-06-15T16:30:29Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5215 subsystem_name=workload_api
  	728:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	732:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	736:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	739:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	742:time="2021-06-15T16:30:29Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	747:time="2021-06-15T16:30:29Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5157 subsystem_name=workload_api
  	751:time="2021-06-15T16:30:33Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	758:time="2021-06-15T16:30:33Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5215 subsystem_name=workload_api
  	761:time="2021-06-15T16:30:33Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	767:time="2021-06-15T16:30:33Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	772:time="2021-06-15T16:30:33Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	775:time="2021-06-15T16:30:33Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	778:time="2021-06-15T16:30:33Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	783:time="2021-06-15T16:30:33Z" level=error msg="Failed to send response" error="rpc error: code = PermissionDenied desc = no identity issued" method=fetch_jwt_bundles pid=5157 subsystem_name=workload_api
  	791:time="2021-06-15T16:30:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	879:time="2021-06-15T16:31:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	977:time="2021-06-15T16:32:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  	1037:time="2021-06-15T16:33:37Z" level=error msg="No identity issued" error="<nil>" registered=false subsystem_name=workload_api
  
  
  ./kind-2.kubeconfig/wcm-system/vl3-nse-green-76bc7f4b96-6qz9k/logs.txt
  
  	52:time="2021-06-15 16:25:16.90420" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	100:time="2021-06-15T16:25:47Z" level=error msg="None of the tls certificates worked: <nil>"
  	102:time="2021-06-15T16:25:47Z" level=error msg="Error occurred while trying to establish connection to: wcmd-green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	245:time="2021-06-15 16:26:47.04580" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: read tcp [::1]:9113->[::1]:53092: read: connection reset by peer\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	388:time="2021-06-15T16:27:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	413:time="2021-06-15T16:27:48Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	439:time="2021-06-15T16:28:18Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	465:time="2021-06-15T16:28:48Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	479:time="2021-06-15T16:29:18Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	493:time="2021-06-15T16:29:48Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	507:time="2021-06-15T16:30:18Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	521:time="2021-06-15T16:30:48Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	535:time="2021-06-15T16:30:48Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	549:time="2021-06-15T16:31:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	563:time="2021-06-15T16:31:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	577:time="2021-06-15T16:32:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	591:time="2021-06-15T16:32:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	605:time="2021-06-15T16:33:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	619:time="2021-06-15T16:33:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	633:time="2021-06-15T16:34:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  
  
  ./kind-2.kubeconfig/wcm-system/vl3-nse-green-76bc7f4b96-wg7rx/logs.txt
  
  	52:time="2021-06-15 16:25:16.53616" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	100:time="2021-06-15T16:25:46Z" level=error msg="None of the tls certificates worked: <nil>"
  	102:time="2021-06-15T16:25:46Z" level=error msg="Error occurred while trying to establish connection to: wcmd-green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	246:time="2021-06-15 16:26:47.20767" level=warning msg="grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"" loc="grpclog/grpclog.go(42)" logger=grpc.grpc-server
  	343:time="2021-06-15T16:27:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	368:time="2021-06-15T16:27:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	394:time="2021-06-15T16:28:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	418:time="2021-06-15T16:28:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	434:time="2021-06-15T16:29:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	448:time="2021-06-15T16:29:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	462:time="2021-06-15T16:30:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	476:time="2021-06-15T16:30:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = Unavailable desc = Connection to Network Registry Server is not available"
  	490:time="2021-06-15T16:30:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	504:time="2021-06-15T16:31:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	518:time="2021-06-15T16:31:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	532:time="2021-06-15T16:32:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	546:time="2021-06-15T16:32:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	560:time="2021-06-15T16:33:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	574:time="2021-06-15T16:33:47Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  	588:time="2021-06-15T16:34:17Z" level=error msg="failed to find network service green@green.nispatil-wcm-cisco.com: rpc error: code = NotFound desc = no NetworkService with name: green"
  
  
  ./kind-2.kubeconfig/wcm-system/wcm-nse-discovery-6fd76df87f-mz5qr/logs.txt
  
  	103:time="2021-06-15T16:26:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	105:time="2021-06-15T16:26:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	106:time="2021-06-15T16:26:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="752c375ac733e25f:01fe2b8534e41b23:752c375ac733e25f:1"
  	109:{"level":"info","ts":1623774407.0512512,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	157:time="2021-06-15T16:26:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	159:time="2021-06-15T16:27:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	160:time="2021-06-15T16:27:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="37e3aece74f1828f:3ddd5c98e15eba6c:37e3aece74f1828f:1"
  	163:{"level":"info","ts":1623774437.1752706,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	213:time="2021-06-15T16:27:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	215:time="2021-06-15T16:27:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	216:time="2021-06-15T16:27:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="24fd19ad41a5a1cf:57ba96c078611b48:24fd19ad41a5a1cf:1"
  	219:{"level":"info","ts":1623774467.2215369,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	267:time="2021-06-15T16:27:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	269:time="2021-06-15T16:28:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	270:time="2021-06-15T16:28:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="7510b6dce30c9f9b:6e24e14f46137a7e:7510b6dce30c9f9b:1"
  	273:{"level":"info","ts":1623774497.2746825,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	321:time="2021-06-15T16:28:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	323:time="2021-06-15T16:28:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	324:time="2021-06-15T16:28:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="2355b437dfe49d51:26c673c182fc4c4c:2355b437dfe49d51:1"
  	327:{"level":"info","ts":1623774527.326386,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	375:time="2021-06-15T16:28:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	377:time="2021-06-15T16:29:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	378:time="2021-06-15T16:29:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="5c192056180b4d41:29d1546efbc34d45:5c192056180b4d41:1"
  	381:{"level":"info","ts":1623774557.3765056,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	431:time="2021-06-15T16:29:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	433:time="2021-06-15T16:29:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	434:time="2021-06-15T16:29:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="3ab45068cb787ba1:222047d93742c8c7:3ab45068cb787ba1:1"
  	437:{"level":"info","ts":1623774587.4138525,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	485:time="2021-06-15T16:29:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	487:time="2021-06-15T16:30:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	488:time="2021-06-15T16:30:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="227440894b49e5db:46ee9cdd43107b86:227440894b49e5db:1"
  	491:{"level":"info","ts":1623774617.474294,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	539:time="2021-06-15T16:30:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	541:time="2021-06-15T16:30:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	542:time="2021-06-15T16:30:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="5981449c96990a81:61cc202c34730bb1:5981449c96990a81:1"
  	545:{"level":"info","ts":1623774647.5241377,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	593:time="2021-06-15T16:30:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	595:time="2021-06-15T16:31:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	596:time="2021-06-15T16:31:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="0e562fad5f7ff832:6004f2ce6ff903d8:0e562fad5f7ff832:1"
  	599:{"level":"info","ts":1623774677.568805,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	649:time="2021-06-15T16:31:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	651:time="2021-06-15T16:31:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	652:time="2021-06-15T16:31:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="2eb213bd04be612a:737a60ce4dbb712d:2eb213bd04be612a:1"
  	655:{"level":"info","ts":1623774707.6062403,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	703:time="2021-06-15T16:31:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	705:time="2021-06-15T16:32:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	706:time="2021-06-15T16:32:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="6406bec6b4f4f5ed:508eb9857facdafe:6406bec6b4f4f5ed:1"
  	709:{"level":"info","ts":1623774737.643773,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	757:time="2021-06-15T16:32:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	759:time="2021-06-15T16:32:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	760:time="2021-06-15T16:32:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="464d125f51223767:5521ce2db5ef9a6b:464d125f51223767:1"
  	763:{"level":"info","ts":1623774767.6809328,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	811:time="2021-06-15T16:32:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	813:time="2021-06-15T16:33:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	814:time="2021-06-15T16:33:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="1a118162be325622:10c35432dc6b8f47:1a118162be325622:1"
  	817:{"level":"info","ts":1623774797.723983,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	867:time="2021-06-15T16:33:27Z" level=error msg="None of the tls certificates worked: <nil>"
  	869:time="2021-06-15T16:33:47Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	870:time="2021-06-15T16:33:47Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="26cbe534f3c4a4b2:54147f87f06b0fd2:26cbe534f3c4a4b2:1"
  	873:{"level":"info","ts":1623774827.762287,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {greenfg8ww green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/greenfg8ww 9f758e8d-8a1a-4313-bab0-cc54f4599e9a 3661 1 2021-06-15 16:26:16 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-wg7rx wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  	921:time="2021-06-15T16:33:57Z" level=error msg="None of the tls certificates worked: <nil>"
  	923:time="2021-06-15T16:34:17Z" level=error msg="Error occurred while trying to establish connection to: green.nispatil-wcm-cisco.com:443. Error: context deadline exceeded"
  	924:time="2021-06-15T16:34:17Z" level=error msg="Failed to dial Network Service Registry at green.nispatil-wcm-cisco.com:443: context deadline exceeded" operation=initRegistryClient span="0dae4206f28ad694:693ae9df71f0ddb8:0dae4206f28ad694:1"
  	927:{"level":"info","ts":1623774857.8011456,"caller":"nseinformer/nseInformerController.go:369","msg":"Error processing pod &{{ } {green8996d green nsm-system /apis/networkservicemesh.io/v1alpha1/namespaces/nsm-system/networkserviceendpoints/green8996d 9d008df7-57fc-49e0-a06e-efbdbe145698 3665 1 2021-06-15 16:26:17 +0000 UTC <nil> <nil> map[app:vl3-nse-green networkservicename:green nsepod.name:green-kind-2_vl3-nse-green-76bc7f4b96-6qz9k wcm/nsr.addr:green.nispatil-wcm-cisco.com wcm/nsr.port:443] map[] [] []  []} {green IP kind-2-control-plane} {RUNNING}}: Connection to Network Registry Server is not available","app":"nse-discovery"}
  
  
  ./kind-2.kubeconfig/wcm-system/wcm-nse-operator-6fb5b6fb4c-28plh/logs.txt
  
  	13:{"level":"info","ts":1623774175.4664543,"logger":"cmd","msg":"Could not create ServiceMonitor object","error":"no ServiceMonitor registered with the API"}
  	14:{"level":"info","ts":1623774175.4665027,"logger":"cmd","msg":"Install prometheus-operator in your cluster to create ServiceMonitor objects","error":"no ServiceMonitor registered with the API"}
  
  
  ./kind-2.kubeconfig/kube-system/kindnet-729mv/logs.txt
  
  	4:I0615 16:10:17.345078       1 main.go:104] Failed to get nodes, retrying after error: Get https://10.96.0.1:443/api/v1/nodes: dial tcp 10.96.0.1:443: i/o timeout
  
  
  ./kind-2.kubeconfig/kube-system/kube-apiserver-kind-2-control-plane/logs.txt
  
  	293:E0615 16:30:26.669133       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.3:33640->172.17.0.3:10250: write: broken pipe
  	294:E0615 16:30:26.671626       1 upgradeaware.go:371] Error proxying data from backend to client: tls: use of closed connection
  	295:E0615 16:30:29.551470       1 upgradeaware.go:357] Error proxying data from client to backend: tls: use of closed connection
  	300:E0615 16:33:21.953450       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.3:40358->172.17.0.3:10250: write: broken pipe
  	301:E0615 16:33:21.953627       1 upgradeaware.go:371] Error proxying data from backend to client: tls: use of closed connection
  	302:E0615 16:33:22.171279       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.3:40392->172.17.0.3:10250: write: broken pipe
  	303:E0615 16:33:22.171525       1 upgradeaware.go:371] Error proxying data from backend to client: tls: use of closed connection
  	304:E0615 16:33:26.131331       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.3:40828->172.17.0.3:10250: write: broken pipe
  	305:E0615 16:33:26.227387       1 upgradeaware.go:357] Error proxying data from client to backend: write tcp 172.17.0.3:40850->172.17.0.3:10250: write: broken pipe
  	306:E0615 16:33:26.704736       1 upgradeaware.go:371] Error proxying data from backend to client: tls: use of closed connection
  
  
  ./kind-2.kubeconfig/kube-system/kube-scheduler-kind-2-control-plane/logs.txt
  
  	6:W0615 16:09:22.361614       1 authentication.go:296] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
  
  
  ./kind-2.kubeconfig/kube-system/coredns-6955765f44-9w8f5/logs.txt
  
  	12:[ERROR] Restart failed: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  	13:[ERROR] plugin/reload: Corefile changed but reload failed: starting with listener file descriptors: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  
  
  ./kind-2.kubeconfig/kube-system/kube-controller-manager-kind-2-control-plane/logs.txt
  
  	10:E0615 16:09:22.365029       1 leaderelection.go:331] error retrieving resource lock kube-system/kube-controller-manager: endpoints "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "endpoints" in API group "" in the namespace "kube-system"
  	183:E0615 16:09:41.161098       1 daemon_controller.go:290] kube-system/kube-proxy failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy", UID:"a619af0e-a533-4314-b235-8e4db63a2064", ResourceVersion:"199", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759370165, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc00041cea0), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-proxy", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc000af9a00), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"xtables-lock", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc00041cf00), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"lib-modules", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc00041cfa0), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"kube-proxy", Image:"k8s.gcr.io/kube-proxy:v1.17.5", Command:[]string{"/usr/local/bin/kube-proxy", "--config=/var/lib/kube-proxy/config.conf", "--hostname-override=$(NODE_NAME)"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc00041d0a0)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-proxy", ReadOnly:false, MountPath:"/var/lib/kube-proxy", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"xtables-lock", ReadOnly:false, MountPath:"/run/xtables.lock", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"lib-modules", ReadOnly:true, MountPath:"/lib/modules", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0006d65a0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000820f98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"beta.kubernetes.io/os":"linux"}, ServiceAccountName:"kube-proxy", DeprecatedServiceAccount:"kube-proxy", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001358b40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"CriticalAddonsOnly", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"system-node-critical", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc0004fc678)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc000820fe8)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:0, NumberReady:0, ObservedGeneration:0, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:0, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kube-proxy": the object has been modified; please apply your changes to the latest version and try again
  	184:E0615 16:09:41.221454       1 daemon_controller.go:290] kube-system/kube-proxy failed with : error storing status for daemon set &v1.DaemonSet{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-proxy", GenerateName:"", Namespace:"kube-system", SelfLink:"/apis/apps/v1/namespaces/kube-system/daemonsets/kube-proxy", UID:"a619af0e-a533-4314-b235-8e4db63a2064", ResourceVersion:"380", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63759370165, loc:(*time.Location)(0x399e9e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string{"deprecated.daemonset.template.generation":"1"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.DaemonSetSpec{Selector:(*v1.LabelSelector)(0xc001823800), Template:v1.PodTemplateSpec{ObjectMeta:v1.ObjectMeta{Name:"", GenerateName:"", Namespace:"", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-proxy"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-proxy", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(0xc00182cfc0), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"xtables-lock", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc001823820), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}, v1.Volume{Name:"lib-modules", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(0xc001823840), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"kube-proxy", Image:"k8s.gcr.io/kube-proxy:v1.17.5", Command:[]string{"/usr/local/bin/kube-proxy", "--config=/var/lib/kube-proxy/config.conf", "--hostname-override=$(NODE_NAME)"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"NODE_NAME", Value:"", ValueFrom:(*v1.EnvVarSource)(0xc001823880)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-proxy", ReadOnly:false, MountPath:"/var/lib/kube-proxy", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"xtables-lock", ReadOnly:false, MountPath:"/run/xtables.lock", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}, v1.VolumeMount{Name:"lib-modules", ReadOnly:true, MountPath:"/lib/modules", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00181e910), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0018e72c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string{"beta.kubernetes.io/os":"linux"}, ServiceAccountName:"kube-proxy", DeprecatedServiceAccount:"kube-proxy", AutomountServiceAccountToken:(*bool)(nil), NodeName:"", HostNetwork:true, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0016856e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"CriticalAddonsOnly", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}, v1.Toleration{Key:"", Operator:"Exists", Value:"", Effect:"", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"system-node-critical", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(nil), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}}, UpdateStrategy:v1.DaemonSetUpdateStrategy{Type:"RollingUpdate", RollingUpdate:(*v1.RollingUpdateDaemonSet)(0xc0006dbb28)}, MinReadySeconds:0, RevisionHistoryLimit:(*int32)(0xc0018e7308)}, Status:v1.DaemonSetStatus{CurrentNumberScheduled:0, NumberMisscheduled:0, DesiredNumberScheduled:1, NumberReady:0, ObservedGeneration:1, UpdatedNumberScheduled:0, NumberAvailable:0, NumberUnavailable:1, CollisionCount:(*int32)(nil), Conditions:[]v1.DaemonSetCondition(nil)}}: Operation cannot be fulfilled on daemonsets.apps "kube-proxy": the object has been modified; please apply your changes to the latest version and try again
  
  
  ./kind-2.kubeconfig/kube-system/coredns-6955765f44-jl66h/logs.txt
  
  	12:[ERROR] Restart failed: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  	13:[ERROR] plugin/reload: Corefile changed but reload failed: starting with listener file descriptors: cannot serve dns://nispatil-wcm-cisco.com.:53 - it is already defined
  
  

